#!/usr/bin/env python3
"""
–ú—É–ª—å—Ç–∏–ø—Ä–æ–≤–∞–π–¥–µ—Ä–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ LLM —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Groq, OpenAI, Anthropic, Ollama
"""

import os
import time
import requests
from typing import Optional, Dict, Any
from dataclasses import dataclass

@dataclass
class LLMResponse:
    """–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM"""
    content: str
    provider: str
    model: str
    tokens_used: Optional[int] = None
    error: Optional[str] = None

class BaseLLMProvider:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    
    def __init__(self, name: str, model: str):
        self.name = name
        self.model = model
        self.is_available = True
        self.last_error = None
    
    def is_configured(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–∞—Å—Ç—Ä–æ–µ–Ω –ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä"""
        raise NotImplementedError
    
    def generate_response(self, system_prompt: str, user_prompt: str) -> LLMResponse:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"""
        raise NotImplementedError
    
    def handle_error(self, error: Exception) -> None:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—à–∏–±–∫—É"""
        self.last_error = str(error)
        if "rate limit" in str(error).lower() or "429" in str(error):
            self.is_available = False
            print(f"‚ö†Ô∏è  {self.name} –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {error}")

class GroqProvider(BaseLLMProvider):
    """Groq LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä"""
    
    def __init__(self):
        super().__init__("Groq", "llama-3.3-70b-versatile")
        self.client = None
        self._init_client()
    
    def _init_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Groq –∫–ª–∏–µ–Ω—Ç"""
        try:
            from groq import Groq
            api_key = os.getenv("GROQ_API_KEY")
            if api_key:
                self.client = Groq(api_key=api_key)
            else:
                self.client = None
        except ImportError:
            self.client = None
    
    def is_configured(self) -> bool:
        return self.client is not None and os.getenv("GROQ_API_KEY") is not None
    
    def generate_response(self, system_prompt: str, user_prompt: str) -> LLMResponse:
        if not self.is_configured():
            return LLMResponse("", self.name, self.model, error="–ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=1500,
            )
            
            content = response.choices[0].message.content
            tokens = response.usage.total_tokens if hasattr(response, 'usage') else None
            
            return LLMResponse(content, self.name, self.model, tokens)
            
        except Exception as e:
            self.handle_error(e)
            return LLMResponse("", self.name, self.model, error=str(e))

class OpenAIProvider(BaseLLMProvider):
    """OpenAI LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä"""
    
    def __init__(self):
        super().__init__("OpenAI", "gpt-3.5-turbo")
        self.client = None
        self._init_client()
    
    def _init_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å OpenAI –∫–ª–∏–µ–Ω—Ç"""
        try:
            import openai
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                self.client = openai.OpenAI(api_key=api_key)
            else:
                self.client = None
        except ImportError:
            self.client = None
    
    def is_configured(self) -> bool:
        return self.client is not None and os.getenv("OPENAI_API_KEY") is not None
    
    def generate_response(self, system_prompt: str, user_prompt: str) -> LLMResponse:
        if not self.is_configured():
            return LLMResponse("", self.name, self.model, error="–ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=1500,
            )
            
            content = response.choices[0].message.content
            tokens = response.usage.total_tokens if hasattr(response, 'usage') else None
            
            return LLMResponse(content, self.name, self.model, tokens)
            
        except Exception as e:
            self.handle_error(e)
            return LLMResponse("", self.name, self.model, error=str(e))

class OllamaProvider(BaseLLMProvider):
    """Ollama –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä"""
    
    def __init__(self):
        super().__init__("Ollama", "llama3.1:8b")
        self.base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    
    def is_configured(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def generate_response(self, system_prompt: str, user_prompt: str) -> LLMResponse:
        if not self.is_configured():
            return LLMResponse("", self.name, self.model, error="Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        try:
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": False,
                "options": {
                    "temperature": 0.3,
                    "num_predict": 1500
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                content = data.get("message", {}).get("content", "")
                tokens = data.get("prompt_eval_count", 0) + data.get("eval_count", 0)
                
                return LLMResponse(content, self.name, self.model, tokens)
            else:
                raise Exception(f"Ollama error: {response.status_code}")
                
        except Exception as e:
            self.handle_error(e)
            return LLMResponse("", self.name, self.model, error=str(e))

class MultiLLMProvider:
    """–ú—É–ª—å—Ç–∏–ø—Ä–æ–≤–∞–π–¥–µ—Ä–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º"""
    
    def __init__(self):
        self.providers = [
            GroqProvider(),
            OpenAIProvider(),
            OllamaProvider(),
        ]
        self.current_provider_index = 0
        self.retry_counts = {provider.name: 0 for provider in self.providers}
        self.max_retries_per_provider = 2
    
    def get_available_providers(self) -> list:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        available = []
        for provider in self.providers:
            if provider.is_configured() and provider.is_available:
                available.append(provider)
        return available
    
    def generate_response(self, system_prompt: str, user_prompt: str, max_retries: int = 3) -> LLMResponse:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º –º–µ–∂–¥—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏"""
        
        available_providers = self.get_available_providers()
        
        if not available_providers:
            return LLMResponse(
                "", 
                "None", 
                "None", 
                error="–í—Å–µ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"
            )
        
        for attempt in range(max_retries):
            # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (–Ω–∞—á–∏–Ω–∞–µ–º —Å —Ç–µ–∫—É—â–µ–≥–æ, –ø–æ—Ç–æ–º –ø–µ—Ä–µ–±–∏—Ä–∞–µ–º)
            for i in range(len(available_providers)):
                provider_index = (self.current_provider_index + i) % len(available_providers)
                provider = available_providers[provider_index]
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å—á–µ—Ä–ø–∞–ª–∏ –ø–æ–ø—ã—Ç–∫–∏
                if self.retry_counts[provider.name] >= self.max_retries_per_provider:
                    continue
                
                print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} —Å {provider.name}...")
                
                response = provider.generate_response(system_prompt, user_prompt)
                
                if response.error:
                    print(f"‚ö†Ô∏è  {provider.name} –æ—à–∏–±–∫–∞: {response.error}")
                    self.retry_counts[provider.name] += 1
                    
                    # –ï—Å–ª–∏ rate limit, –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
                    if "rate limit" in response.error.lower() or "429" in response.error:
                        provider.is_available = False
                        print(f"üö´ {provider.name} –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω –∏–∑-–∑–∞ rate limit")
                    
                    continue
                
                # –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç
                print(f"‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç {provider.name}")
                self.current_provider_index = provider_index
                self.retry_counts[provider.name] = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
                
                return response
            
            # –ï—Å–ª–∏ –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∂–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
            if attempt < max_retries - 1:
                delay = 2 ** attempt  # Exponential backoff
                print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {delay} —Å–µ–∫ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                time.sleep(delay)
        
        # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
        return LLMResponse(
            "", 
            "None", 
            "None", 
            error="–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã. –í—Å–µ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
        )
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        status = {}
        for provider in self.providers:
            status[provider.name] = {
                "configured": provider.is_configured(),
                "available": provider.is_available,
                "retry_count": self.retry_counts[provider.name],
                "last_error": provider.last_error
            }
        return status
    
    def reset_provider(self, provider_name: str) -> None:
        """–°–±—Ä–æ—Å–∏—Ç—å —Å—Ç–∞—Ç—É—Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (–¥–ª—è —Ä—É—á–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è)"""
        for provider in self.providers:
            if provider.name == provider_name:
                provider.is_available = True
                self.retry_counts[provider.name] = 0
                provider.last_error = None
                print(f"üîÑ {provider_name} —Å–±—Ä–æ—à–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º—É–ª—å—Ç–∏–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
multi_llm = MultiLLMProvider()

def get_llm_response(system_prompt: str, user_prompt: str) -> Optional[str]:
    """–£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"""
    response = multi_llm.generate_response(system_prompt, user_prompt)
    
    if response.error:
        print(f"‚ùå LLM –æ—à–∏–±–∫–∞: {response.error}")
        return None
    
    return response.content

def get_llm_status() -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    return multi_llm.get_status()

#!/usr/bin/env python3
"""
–ú—É–ª—å—Ç–∏–ø—Ä–æ–≤–∞–π–¥–µ—Ä–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ LLM —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Groq, OpenAI, Anthropic, Ollama
"""

import os
import time
import requests
from typing import Optional, Dict, Any
from dataclasses import dataclass

@dataclass
class LLMResponse:
    """–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM"""
    content: str
    provider: str
    model: str
    tokens_used: Optional[int] = None
    error: Optional[str] = None

class BaseLLMProvider:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    
    def __init__(self, name: str, model: str):
        self.name = name
        self.model = model
        self.is_available = True
        self.last_error = None
    
    def is_configured(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–∞—Å—Ç—Ä–æ–µ–Ω –ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä"""
        raise NotImplementedError
    
    def generate_response(self, system_prompt: str, user_prompt: str) -> LLMResponse:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"""
        raise NotImplementedError
    
    def handle_error(self, error: Exception) -> None:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—à–∏–±–∫—É"""
        self.last_error = str(error)
        if "rate limit" in str(error).lower() or "429" in str(error):
            self.is_available = False
            print(f"‚ö†Ô∏è  {self.name} –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {error}")

class GroqProvider(BaseLLMProvider):
    """Groq LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä"""
    
    def __init__(self):
        super().__init__("Groq", "llama-3.3-70b-versatile")
        self.client = None
        self._init_client()
    
    def _init_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Groq –∫–ª–∏–µ–Ω—Ç"""
        try:
            from groq import Groq
            api_key = os.getenv("GROQ_API_KEY")
            if api_key:
                self.client = Groq(api_key=api_key)
            else:
                self.client = None
        except ImportError:
            self.client = None
    
    def is_configured(self) -> bool:
        return self.client is not None and os.getenv("GROQ_API_KEY") is not None
    
    def generate_response(self, system_prompt: str, user_prompt: str) -> LLMResponse:
        if not self.is_configured():
            return LLMResponse("", self.name, self.model, error="–ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=1500,
            )
            
            content = response.choices[0].message.content
            tokens = response.usage.total_tokens if hasattr(response, 'usage') else None
            
            return LLMResponse(content, self.name, self.model, tokens)
            
        except Exception as e:
            self.handle_error(e)
            return LLMResponse("", self.name, self.model, error=str(e))

class OpenAIProvider(BaseLLMProvider):
    """OpenAI LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä"""
    
    def __init__(self):
        super().__init__("OpenAI", "gpt-3.5-turbo")
        self.client = None
        self._init_client()
    
    def _init_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å OpenAI –∫–ª–∏–µ–Ω—Ç"""
        try:
            import openai
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                self.client = openai.OpenAI(api_key=api_key)
            else:
                self.client = None
        except ImportError:
            self.client = None
    
    def is_configured(self) -> bool:
        return self.client is not None and os.getenv("OPENAI_API_KEY") is not None
    
    def generate_response(self, system_prompt: str, user_prompt: str) -> LLMResponse:
        if not self.is_configured():
            return LLMResponse("", self.name, self.model, error="–ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=1500,
            )
            
            content = response.choices[0].message.content
            tokens = response.usage.total_tokens if hasattr(response, 'usage') else None
            
            return LLMResponse(content, self.name, self.model, tokens)
            
        except Exception as e:
            self.handle_error(e)
            return LLMResponse("", self.name, self.model, error=str(e))

class OllamaProvider(BaseLLMProvider):
    """Ollama –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä"""
    
    def __init__(self):
        super().__init__("Ollama", "llama3.1:8b")
        self.base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    
    def is_configured(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def generate_response(self, system_prompt: str, user_prompt: str) -> LLMResponse:
        if not self.is_configured():
            return LLMResponse("", self.name, self.model, error="Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        try:
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": False,
                "options": {
                    "temperature": 0.3,
                    "num_predict": 1500
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                content = data.get("message", {}).get("content", "")
                tokens = data.get("prompt_eval_count", 0) + data.get("eval_count", 0)
                
                return LLMResponse(content, self.name, self.model, tokens)
            else:
                raise Exception(f"Ollama error: {response.status_code}")
                
        except Exception as e:
            self.handle_error(e)
            return LLMResponse("", self.name, self.model, error=str(e))

class MultiLLMProvider:
    """–ú—É–ª—å—Ç–∏–ø—Ä–æ–≤–∞–π–¥–µ—Ä–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º"""
    
    def __init__(self):
        self.providers = [
            GroqProvider(),
            OpenAIProvider(),
            OllamaProvider(),
        ]
        self.current_provider_index = 0
        self.retry_counts = {provider.name: 0 for provider in self.providers}
        self.max_retries_per_provider = 2
        self.last_reset_time = {provider.name: 0 for provider in self.providers}
        self.reset_interval = 3600  # 1 —á–∞—Å –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    
    def _should_reset_provider(self, provider_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω—É–∂–Ω–æ –ª–∏ —Å–±—Ä–æ—Å–∏—Ç—å —Å—Ç–∞—Ç—É—Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        import time
        current_time = time.time()
        last_reset = self.last_reset_time.get(provider_name, 0)
        return (current_time - last_reset) > self.reset_interval
    
    def _auto_reset_providers(self):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–±—Ä–æ—Å–∏—Ç—å —Å—Ç–∞—Ç—É—Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è"""
        import time
        current_time = time.time()
        
        for provider in self.providers:
            if not provider.is_available and self._should_reset_provider(provider.name):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                try:
                    # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
                    if provider.name == "Groq" and provider.is_configured():
                        # –î–ª—è Groq –¥–µ–ª–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
                        test_response = provider.generate_response(
                            "Test", "Say 'test'", max_retries=1
                        )
                        if not test_response.error or "rate limit" not in test_response.error.lower():
                            provider.is_available = True
                            self.retry_counts[provider.name] = 0
                            provider.last_error = None
                            self.last_reset_time[provider.name] = current_time
                            print(f"üîÑ {provider.name} –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                    
                except Exception as e:
                    # –ï—Å–ª–∏ —Ç–µ—Å—Ç –Ω–µ –ø—Ä–æ—à–µ–ª, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    pass

    def get_available_providers(self) -> list:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—Ä–µ–º–µ–Ω–∏
        self._auto_reset_providers()
        
        available = []
        for provider in self.providers:
            if provider.is_configured() and provider.is_available:
                available.append(provider)
        return available
    
    def generate_response(self, system_prompt: str, user_prompt: str, max_retries: int = 3) -> LLMResponse:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º –º–µ–∂–¥—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏"""
        
        available_providers = self.get_available_providers()
        
        if not available_providers:
            return LLMResponse(
                "", 
                "None", 
                "None", 
                error="–í—Å–µ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"
            )
        
        for attempt in range(max_retries):
            # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (–Ω–∞—á–∏–Ω–∞–µ–º —Å —Ç–µ–∫—É—â–µ–≥–æ, –ø–æ—Ç–æ–º –ø–µ—Ä–µ–±–∏—Ä–∞–µ–º)
            for i in range(len(available_providers)):
                provider_index = (self.current_provider_index + i) % len(available_providers)
                provider = available_providers[provider_index]
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å—á–µ—Ä–ø–∞–ª–∏ –ø–æ–ø—ã—Ç–∫–∏
                if self.retry_counts[provider.name] >= self.max_retries_per_provider:
                    continue
                
                print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} —Å {provider.name}...")
                
                response = provider.generate_response(system_prompt, user_prompt)
                
                if response.error:
                    print(f"‚ö†Ô∏è  {provider.name} –æ—à–∏–±–∫–∞: {response.error}")
                    self.retry_counts[provider.name] += 1
                    
                    # –ï—Å–ª–∏ rate limit, –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
                    if "rate limit" in response.error.lower() or "429" in response.error:
                        provider.is_available = False
                        print(f"üö´ {provider.name} –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω –∏–∑-–∑–∞ rate limit")
                    
                    continue
                
                # –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç
                print(f"‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç {provider.name}")
                self.current_provider_index = provider_index
                self.retry_counts[provider.name] = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
                
                return response
            
            # –ï—Å–ª–∏ –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∂–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
            if attempt < max_retries - 1:
                delay = 2 ** attempt  # Exponential backoff
                print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {delay} —Å–µ–∫ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                time.sleep(delay)
        
        # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
        return LLMResponse(
            "", 
            "None", 
            "None", 
            error="–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã. –í—Å–µ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
        )
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        status = {}
        for provider in self.providers:
            status[provider.name] = {
                "configured": provider.is_configured(),
                "available": provider.is_available,
                "retry_count": self.retry_counts[provider.name],
                "last_error": provider.last_error
            }
        return status
    
    def reset_provider(self, provider_name: str) -> None:
        """–°–±—Ä–æ—Å–∏—Ç—å —Å—Ç–∞—Ç—É—Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (–¥–ª—è —Ä—É—á–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è)"""
        for provider in self.providers:
            if provider.name == provider_name:
                provider.is_available = True
                self.retry_counts[provider.name] = 0
                provider.last_error = None
                print(f"üîÑ {provider_name} —Å–±—Ä–æ—à–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º—É–ª—å—Ç–∏–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
multi_llm = MultiLLMProvider()

def get_llm_response(system_prompt: str, user_prompt: str) -> Optional[str]:
    """–£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"""
    response = multi_llm.generate_response(system_prompt, user_prompt)
    
    if response.error:
        print(f"‚ùå LLM –æ—à–∏–±–∫–∞: {response.error}")
        return None
    
    return response.content

def get_llm_status() -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    return multi_llm.get_status()

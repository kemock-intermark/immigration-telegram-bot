#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å BM25 –ø–æ–∏—Å–∫–æ–º
–í–µ—Ä—Å–∏—è 3.0 - –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±–µ–∑ –∫–æ—Å—Ç—ã–ª–µ–π
"""

import json
import re
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
from functools import lru_cache

# BM25 –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
try:
    from rank_bm25 import BM25Okapi
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False
    print("‚ö†Ô∏è  rank_bm25 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install rank-bm25")

# LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
try:
    from groq import Groq
    import os
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    print("‚ö†Ô∏è  Groq –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")


class TextNormalizer:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞"""
    
    def __init__(self):
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ (–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è lite)
        self.replacements = {
            # –ü–∞–¥–µ–∂–∏ —Å—Ç—Ä–∞–Ω (–æ–∫–æ–Ω—á–∞–Ω–∏—è)
            r'–ø–æ—Ä—Ç—É–≥–∞–ª–∏\w+': 'portugal',
            r'–≥—Ä–µ—Ü–∏\w+': 'greece',
            r'—Ç—É—Ä—Ü–∏\w+': 'turkey',
            r'–≥—Ä–µ–Ω–∞–¥\w+': 'grenada',
            r'–º–∞–ª—å—Ç\w+': 'malta',
            r'–∫–∏–ø—Ä\w*': 'cyprus',
            r'–∏—Å–ø–∞–Ω–∏\w+': 'spain',
            r'–ø–∞—Ä–∞–≥–≤–∞\w+': 'paraguay',
            r'–≤–∞–Ω—É–∞—Ç—É': 'vanuatu',
            r'–¥–æ–º–∏–Ω–∏–∫\w+': 'dominica',
            r'–∞–Ω—Ç–∏–≥—É–∞': 'antigua',
            r'–±–∞—Ä–±—É–¥\w+': 'barbuda',
            r'–∫–∞—Ä–∏–±—Å–∫\w+': 'caribbean',
            r'—Å–µ–Ω—Ç[\s\-]–∫–∏—Ç—Ç—Å': 'saint kitts',
            r'—Å–µ–Ω—Ç[\s\-]–∫–∏—Ç—Å': 'saint kitts',
            r'—Å–µ–Ω—Ç[\s\-]–ª—é—Å–∏\w+': 'saint lucia',
            r'—Å–µ–Ω—Ç[\s\-]–ª—é—á–∏\w+': 'saint lucia',
            
            # –ü—Ä–æ–≥—Ä–∞–º–º—ã –∏–º–º–∏–≥—Ä–∞—Ü–∏–∏
            r'–≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤\w+': 'citizenship',
            r'–ø–∞—Å–ø–æ—Ä—Ç\w*': 'passport',
            r'–≤–∏–∑\w+': 'visa',
            r'–≤–Ω–∂': 'residence_permit',
            r'–≤–∏–¥\s+–Ω–∞\s+–∂–∏—Ç–µ–ª—å—Å—Ç–≤\w*': 'residence_permit',
            r'–ø–æ—Å—Ç–æ—è–Ω–Ω\w+\s+–ø—Ä–æ–∂–∏–≤–∞–Ω\w+': 'permanent_residence',
            r'–ø–º–∂': 'permanent_residence',
            r'–∏–Ω–≤–µ—Å—Ç–∏—Ü\w+': 'investment',
            r'–≥–æ–ª–¥–µ–Ω\w*': 'golden',
            r'–∑–æ–ª–æ—Ç\w+': 'golden',
            
            # –ì–ª–∞–≥–æ–ª—ã
            r'–ø–æ–ª—É—á\w+': 'get',
            r'–æ—Ñ–æ—Ä–º\w+': 'apply',
            r'—Ç—Ä–µ–±–æ–≤–∞–Ω\w+': 'requirements',
            r'—É—Å–ª–æ–≤\w+': 'conditions',
            r'—Å—Ç–æ–∏–º–æ—Å—Ç\w+': 'cost',
            r'—Ü–µ–Ω\w+': 'price',
            r'—Å—Ä–æ–∫\w*': 'timeline',
        }
        
    def normalize(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç"""
        text = text.lower()
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–º–µ–Ω—ã
        for pattern, replacement in self.replacements.items():
            text = re.sub(pattern, replacement, text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def tokenize(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏—Ç—å –Ω–∞ —Ç–æ–∫–µ–Ω—ã"""
        normalized = self.normalize(text)
        tokens = normalized.split()
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (—Å—Ç–æ–ø-—Å–ª–æ–≤–∞)
        tokens = [t for t in tokens if len(t) > 2]
        return tokens


class KnowledgeAgentV3:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç —Å BM25 –ø–æ–∏—Å–∫–æ–º"""
    
    def __init__(self, knowledge_dir: str):
        self.knowledge_dir = Path(knowledge_dir)
        self.documents = []
        self.normalizer = TextNormalizer()
        self.bm25 = None
        self.tokenized_corpus = []
        self.kb_version = None
        
        # LLM –∫–ª–∏–µ–Ω—Ç
        self.groq_client = None
        if GROQ_AVAILABLE:
            groq_api_key = os.getenv('GROQ_API_KEY')
            if groq_api_key:
                try:
                    self.groq_client = Groq(api_key=groq_api_key)
                except Exception as e:
                    print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Groq: {e}")
        
        self.load_knowledge_base()
    
    def load_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Ä—Å–∏—é –∏–∑ manifest –µ—Å–ª–∏ –µ—Å—Ç—å
        manifest_path = self.knowledge_dir / 'manifest.json'
        if manifest_path.exists():
            try:
                with open(manifest_path, 'r', encoding='utf-8') as f:
                    manifest = json.load(f)
                    self.kb_version = manifest.get('version', 'unknown')
                    print(f"üì¶ –í–µ—Ä—Å–∏—è –±–∞–∑—ã: {self.kb_version}")
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ manifest: {e}")
                self.kb_version = 'unknown'
        else:
            self.kb_version = 'unknown'
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ .md —Ñ–∞–π–ª—ã –∫—Ä–æ–º–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö
        md_files = []
        for md_file in self.knowledge_dir.rglob("*.md"):
            if not md_file.name.startswith(('00_', '_')):
                md_files.append(md_file)
        
        for md_file in md_files:
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                doc = self.extract_metadata_and_content(content, md_file)
                if doc:
                    self.documents.append(doc)
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {md_file.name}: {e}")
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –°—Ç—Ä–æ–∏–º BM25 –∏–Ω–¥–µ–∫—Å
        if BM25_AVAILABLE and self.documents:
            print("üîç –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞...")
            self.build_bm25_index()
            print("‚úÖ BM25 –∏–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤")
    
    def extract_metadata_and_content(self, content: str, file_path: Path) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç"""
        try:
            # –†–∞–∑–¥–µ–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —Ç–µ–ª–æ
            if content.startswith('---'):
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    metadata_str = parts[1]
                    body = parts[2].strip()
                else:
                    return None
            else:
                return None
            
            # –ü–∞—Ä—Å–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            doc = {'body': body, 'file_path': str(file_path)}
            
            for line in metadata_str.split('\n'):
                line = line.strip()
                if ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip().strip('"').strip("'")
                    
                    if key in ['title', 'summary', 'category', 'subcategory']:
                        doc[key] = value
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º source —Ñ–∞–π–ª—ã
            source_match = re.search(r'- path:\s*"([^"]+)"', metadata_str)
            if source_match:
                doc['source_file'] = source_match.group(1).replace('raw/', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–∞–π–¥—ã
            slides_match = re.search(r'slides:\s*\[(\d+)-(\d+)\]', metadata_str)
            if slides_match:
                doc['slides_start'] = slides_match.group(1)
                doc['slides_end'] = slides_match.group(2)
            
            return doc
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {file_path.name}: {e}")
            return None
    
    def build_bm25_index(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å BM25 –∏–Ω–¥–µ–∫—Å"""
        if not BM25_AVAILABLE:
            print("‚ùå BM25 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        self.tokenized_corpus = []
        for doc in self.documents:
            # –í–ê–ñ–ù–û: –ü–æ–≤—Ç–æ—Ä—è–µ–º title –∏ category 3 —Ä–∞–∑–∞ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∏—Ö –≤–µ—Å–∞
            # –≠—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –¥–ª—è boosting –≤–∞–∂–Ω—ã—Ö –ø–æ–ª–µ–π
            title = doc.get('title', '')
            category = doc.get('category', '')
            subcategory = doc.get('subcategory', '')
            
            text_parts = [
                title, title, title,  # 3x boost –¥–ª—è title
                category, category,   # 2x boost –¥–ª—è category
                subcategory,
                doc.get('summary', ''),
                doc.get('body', '')[:5000]  # –ü–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–ª–∞
            ]
            
            full_text = ' '.join(text_parts)
            tokens = self.normalizer.tokenize(full_text)
            self.tokenized_corpus.append(tokens)
        
        # –°–æ–∑–¥–∞—ë–º BM25 –∏–Ω–¥–µ–∫—Å
        self.bm25 = BM25Okapi(self.tokenized_corpus)
    
    def search_documents(self, query: str, limit: int = 5) -> List[Tuple[Dict, float]]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å BM25"""
        if not BM25_AVAILABLE or not self.bm25:
            print("‚ùå BM25 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
            return self._fallback_search(query, limit)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_tokens = self.normalizer.tokenize(query)
        query_normalized = ' '.join(query_tokens)
        
        # BM25 —Å–∫–æ—Ä–∏–Ω–≥
        scores = self.bm25.get_scores(query_tokens)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        results = []
        for idx, score in enumerate(scores):
            if score > 0:
                doc = self.documents[idx]
                final_score = float(score)
                
                # –ë–û–ù–£–°: —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–∞–∂–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ title/category
                title_normalized = self.normalizer.normalize(doc.get('title', ''))
                category_normalized = self.normalizer.normalize(doc.get('category', ''))
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –≤ –∑–∞–ø—Ä–æ—Å–µ
                for token in query_tokens:
                    if len(token) > 3:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
                        if token in title_normalized.split():
                            final_score *= 1.5  # 50% –±—É—Å—Ç –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ title
                        elif token in category_normalized.split():
                            final_score *= 1.2  # 20% –±—É—Å—Ç –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ category
                
                results.append((doc, final_score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results[:limit]
    
    def _fallback_search(self, query: str, limit: int = 5) -> List[Tuple[Dict, float]]:
        """–ü—Ä–æ—Å—Ç–æ–π fallback –ø–æ–∏—Å–∫ –µ—Å–ª–∏ BM25 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        query_lower = query.lower()
        query_words = set(query_lower.split())
        
        results = []
        for doc in self.documents:
            body_lower = doc.get('body', '').lower()
            title_lower = doc.get('title', '').lower()
            
            score = 0
            for word in query_words:
                if len(word) > 2:
                    if word in title_lower:
                        score += 10
                    if word in body_lower:
                        score += 1
            
            if score > 0:
                results.append((doc, score))
        
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:limit]
    
    def extract_relevant_content(self, doc: Dict, query: str, context_size: int = 6000) -> str:
        """–ò–∑–≤–ª–µ—á—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        body = doc.get('body', '')
        title = doc.get('title', '')
        summary = doc.get('summary', '')
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = f"# {title}\n\n"
        if summary:
            context += f"**–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:** {summary}\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–µ—Ä–≤—ã–µ —Å–ª–∞–π–¥—ã)
        if body:
            slides = body.split('--- –°–ª–∞–π–¥')
            relevant_slides = []
            for i, slide in enumerate(slides[:15]):
                if slide.strip():
                    if i > 0:
                        relevant_slides.append('--- –°–ª–∞–π–¥' + slide)
                    else:
                        relevant_slides.append(slide)
            
            body_excerpt = '\n'.join(relevant_slides)
            if len(body_excerpt) > context_size:
                body_excerpt = body_excerpt[:context_size] + "\n\n[...–¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è...]"
            
            context += body_excerpt
        
        return context
    
    def generate_llm_answer(self, query: str, context: str, sources: List[str]) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é LLM"""
        if not self.groq_client:
            return None
        
        try:
            system_prompt = """–¢—ã - –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∞–º –∏–º–º–∏–≥—Ä–∞—Ü–∏–∏ –∏ –ø–æ–ª—É—á–µ–Ω–∏—è –≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–∞.
–í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
1. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ - —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ "–Ω–µ—Ç –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö"
3. –í—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, —Å—É–º–º—ã, —Å—Ä–æ–∫–∏ –∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
4. –§–æ—Ä–º–∞—Ç–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ —Å–ø–∏—Å–∫–∞–º–∏
5. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, —á—Ç–æ –∏ –≤–æ–ø—Ä–æ—Å (—Ä—É—Å—Å–∫–∏–π –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
6. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é - —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""

            user_prompt = f"""–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
{query}

–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:
{context}

–î–∞–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º."""

            response = self.groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=1500,
            )
            
            llm_answer = response.choices[0].message.content
            
            final_answer = llm_answer + "\n\n---\n\n**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n"
            for source in sources:
                final_answer += f"- {source}\n"
            
            return final_answer
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ LLM: {e}")
            return None
    
    def format_answer(self, query: str, results: List[Tuple[Dict, float]]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"""
        if not results:
            return "‚ùå –ù–µ –∑–Ω–∞—é ‚Äî –Ω–µ—Ç –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö.\n\n–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        contexts = []
        sources = []
        
        for doc, score in results[:3]:
            context = self.extract_relevant_content(doc, query)
            contexts.append(context)
            
            source_file = doc.get('source_file', 'Unknown')
            slides_start = doc.get('slides_start', '?')
            slides_end = doc.get('slides_end', '?')
            source_line = f"raw/{source_file} ‚Üí —Å–ª–∞–π–¥—ã {slides_start}‚Äì{slides_end}"
            if source_line not in sources:
                sources.append(source_line)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LLM
        if self.groq_client and contexts:
            full_context = "\n\n---\n\n".join(contexts)
            llm_answer = self.generate_llm_answer(query, full_context, sources)
            if llm_answer:
                return "\n\n## –û—Ç–≤–µ—Ç\n\n" + llm_answer
        
        # Fallback –µ—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        answer = "\n\n## –û—Ç–≤–µ—Ç\n\n" + contexts[0]
        answer += "\n\n---\n\n**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n"
        for source in sources:
            answer += f"- {source}\n"
        
        return answer
    
    def ask(self, question: str) -> str:
        """–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å"""
        results = self.search_documents(question, limit=5)
        answer = self.format_answer(question, results)
        return answer


# –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
KnowledgeAgent = KnowledgeAgentV3


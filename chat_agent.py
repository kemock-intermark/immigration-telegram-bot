#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å BM25 –ø–æ–∏—Å–∫–æ–º
–í–µ—Ä—Å–∏—è 3.0 - –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±–µ–∑ –∫–æ—Å—Ç—ã–ª–µ–π
–í–µ—Ä—Å–∏—è 3.1 - –¥–≤—É—è–∑—ã—á–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ (rus/eng)
"""

import json
import re
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Literal
from collections import defaultdict
from functools import lru_cache

# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–∞–º–∏
try:
    from language_utils import LanguageDetector, LanguageRouter, Language
    LANGUAGE_UTILS_AVAILABLE = True
except ImportError:
    LANGUAGE_UTILS_AVAILABLE = False
    Language = Literal["rus", "eng"]
    print("‚ö†Ô∏è  language_utils –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–≤—É—è–∑—ã—á–Ω–æ—Å—Ç—å –æ—Ç–∫–ª—é—á–µ–Ω–∞.")

# BM25 –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
try:
    from rank_bm25 import BM25Okapi
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False
    print("‚ö†Ô∏è  rank_bm25 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install rank-bm25")

# LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
try:
    from groq import Groq
    import os
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    print("‚ö†Ô∏è  Groq –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")


class TextNormalizer:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞"""
    
    def __init__(self):
        # –î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –ø–µ—Ä–µ–≤–æ–¥–æ–≤: RU ‚Üî EN
        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–∫–∞—Ç—å –Ω–∞ –ª—é–±–æ–º —è–∑—ã–∫–µ
        self.translations = {
            # –°—Ç—Ä–∞–Ω—ã (–∫–æ—Ä–µ–Ω—å —Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ ‚Üí –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
            'portugal': ['–ø–æ—Ä—Ç—É–≥–∞–ª', 'portugal'],
            'greece': ['–≥—Ä–µ—Ü', '–≥—Ä–µ—á', '–≥—Ä–µ—á–µ—Å–∫', 'greece'],
            'turkey': ['—Ç—É—Ä—Ü', '—Ç—É—Ä–µ—Ü', '—Ç—É—Ä–µ—Ü–∫', 'turkey'],
            'grenada': ['–≥—Ä–µ–Ω–∞–¥', 'grenada'],
            'malta': ['–º–∞–ª—å—Ç', 'malta'],
            'cyprus': ['–∫–∏–ø—Ä', 'cyprus'],
            'spain': ['–∏—Å–ø–∞–Ω', 'spain'],
            'paraguay': ['–ø–∞—Ä–∞–≥–≤–∞', 'paraguay'],
            'vanuatu': ['–≤–∞–Ω—É–∞—Ç—É', 'vanuatu'],
            'dominica': ['–¥–æ–º–∏–Ω–∏–∫', 'dominica'],
            'antigua': ['–∞–Ω—Ç–∏–≥—É–∞', 'antigua'],
            'barbuda': ['–±–∞—Ä–±—É–¥', 'barbuda'],
            'caribbean': ['–∫–∞—Ä–∏–±—Å–∫', 'caribbean'],
            'saint kitts': ['—Å–µ–Ω—Ç –∫–∏—Ç—Ç—Å', '—Å–µ–Ω—Ç –∫–∏—Ç—Å', 'saint kitts', 'st kitts'],
            'saint lucia': ['—Å–µ–Ω—Ç –ª—é—Å–∏', '—Å–µ–Ω—Ç –ª—é—á–∏', 'saint lucia', 'st lucia'],
            'france': ['—Ñ—Ä–∞–Ω—Ü', 'france'],
            'germany': ['–≥–µ—Ä–º–∞–Ω', 'germany', 'deutschland'],
            'italy': ['–∏—Ç–∞–ª', 'italy'],
            'austria': ['–∞–≤—Å—Ç—Ä', 'austria'],
            'hungary': ['–≤–µ–Ω–≥—Ä', 'hungary'],
            'bulgaria': ['–±–æ–ª–≥–∞—Ä', 'bulgaria'],
            'serbia': ['—Å–µ—Ä–±', 'serbia'],
            'montenegro': ['—á–µ—Ä–Ω–æ–≥–æ—Ä', 'montenegro'],
            'latvia': ['–ª–∞—Ç–≤', 'latvia'],
            'slovenia': ['—Å–ª–æ–≤–µ–Ω', 'slovenia'],
            'luxembourg': ['–ª—é–∫—Å–µ–º–±—É—Ä–≥', 'luxembourg'],
            'canada': ['–∫–∞–Ω–∞–¥', 'canada'],
            'usa': ['—Å—à–∞', 'usa', '–∞–º–µ—Ä–∏–∫'],
            'uk': ['–≤–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω', '–±—Ä–∏—Ç–∞–Ω', 'uk', 'united kingdom'],
            'egypt': ['–µ–≥–∏–ø—Ç', 'egypt'],
            'nauru': ['–Ω–∞—É—Ä—É', 'nauru'],
            'sao tome': ['—Å–∞–Ω —Ç–æ–º', 'sao tome'],
            
            # –ü—Ä–æ–≥—Ä–∞–º–º—ã –∏–º–º–∏–≥—Ä–∞—Ü–∏–∏
            'citizenship': ['–≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤', 'citizenship', '–ø–∞—Å–ø–æ—Ä—Ç', 'passport'],
            'residence permit': ['–≤–Ω–∂', '–≤–∏–¥ –Ω–∞ –∂–∏—Ç–µ–ª—å—Å—Ç–≤', 'residence permit', '–≤–∏–∑', 'visa'],
            'permanent residence': ['–ø–º–∂', '–ø–æ—Å—Ç–æ—è–Ω–Ω –ø—Ä–æ–∂–∏–≤–∞–Ω', 'permanent residence'],
            'golden visa': ['–∑–æ–ª–æ—Ç –≤–∏–∑', '–≥–æ–ª–¥–µ–Ω –≤–∏–∑', 'golden visa'],
            'investment': ['–∏–Ω–≤–µ—Å—Ç–∏—Ü', 'investment'],
            'digital nomad': ['—Ü–∏—Ñ—Ä–æ–≤ –∫–æ—á–µ–≤–Ω–∏–∫', 'digital nomad'],
            'startup': ['—Å—Ç–∞—Ä—Ç–∞–ø', 'startup'],
            'business': ['–±–∏–∑–Ω–µ—Å', 'business'],
            
            # –û–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
            'cost': ['—Å—Ç–æ–∏–º–æ—Å—Ç', '—Ü–µ–Ω', 'cost', 'price'],
            'requirements': ['—Ç—Ä–µ–±–æ–≤–∞–Ω', '—É—Å–ª–æ–≤', 'requirements', 'conditions'],
            'timeline': ['—Å—Ä–æ–∫', 'timeline'],
        }
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∑–∞–º–µ–Ω—ã (—Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞, –Ω–æ —Ç–µ–ø–µ—Ä—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        self.replacements = {}
        for english_key, variants in self.translations.items():
            for variant in variants:
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω —Å –æ–∫–æ–Ω—á–∞–Ω–∏—è–º–∏
                pattern = variant.replace(' ', r'\s+') + r'\w*'
                self.replacements[pattern] = english_key.replace(' ', '_')
        
    def normalize(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç"""
        text = text.lower()
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–º–µ–Ω—ã
        for pattern, replacement in self.replacements.items():
            text = re.sub(pattern, replacement, text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def tokenize(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏—Ç—å –Ω–∞ —Ç–æ–∫–µ–Ω—ã"""
        normalized = self.normalize(text)
        tokens = normalized.split()
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (—Å—Ç–æ–ø-—Å–ª–æ–≤–∞)
        tokens = [t for t in tokens if len(t) > 2]
        return tokens


class KnowledgeAgentV3:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç —Å BM25 –ø–æ–∏—Å–∫–æ–º –∏ –¥–≤—É—è–∑—ã—á–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π"""
    
    def __init__(self, knowledge_dir: str, lang: Optional[Language] = None, auto_detect_lang: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
        
        Args:
            knowledge_dir: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ knowledge/
            lang: –Ø–∑—ã–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ ("rus" –∏–ª–∏ "eng"). –ï—Å–ª–∏ None - –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –æ–±–∞
            auto_detect_lang: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å —è–∑—ã–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        """
        self.knowledge_dir = Path(knowledge_dir)
        self.documents = []
        self.normalizer = TextNormalizer()
        self.bm25 = None
        self.tokenized_corpus = []
        self.kb_version = None
        self.lang = lang  # –¢–µ–∫—É—â–∏–π —è–∑—ã–∫ (–∏–ª–∏ None –¥–ª—è –æ–±–æ–∏—Ö)
        self.auto_detect_lang = auto_detect_lang
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö —É—Ç–∏–ª–∏—Ç
        if LANGUAGE_UTILS_AVAILABLE:
            self.language_detector = LanguageDetector()
            self.language_router = LanguageRouter(self.knowledge_dir)
        else:
            self.language_detector = None
            self.language_router = None
        
        # LLM –∫–ª–∏–µ–Ω—Ç
        self.groq_client = None
        if GROQ_AVAILABLE:
            groq_api_key = os.getenv('GROQ_API_KEY')
            if groq_api_key:
                try:
                    self.groq_client = Groq(api_key=groq_api_key)
                except Exception as e:
                    print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Groq: {e}")
        
        self.load_knowledge_base()
    
    def load_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å —É—á–µ—Ç–æ–º —è–∑—ã–∫–∞"""
        lang_label = f" ({self.lang.upper()})" if self.lang else ""
        print(f"üìö –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π{lang_label}...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Ä—Å–∏—é –∏–∑ manifest (—Å —É—á–µ—Ç–æ–º —è–∑—ã–∫–∞)
        self._load_manifest()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        search_dirs = self._get_search_directories()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ .md —Ñ–∞–π–ª—ã
        md_files = []
        for search_dir in search_dirs:
            for md_file in search_dir.rglob("*.md"):
                if not md_file.name.startswith(('00_', '_')):
                    md_files.append(md_file)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        for md_file in md_files:
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                doc = self.extract_metadata_and_content(content, md_file)
                if doc:
                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —è–∑—ã–∫—É (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω)
                    if self.lang is None or doc.get('lang') == self.lang:
                        self.documents.append(doc)
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {md_file.name}: {e}")
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤{lang_label}")
        
        # –°—Ç—Ä–æ–∏–º BM25 –∏–Ω–¥–µ–∫—Å
        if BM25_AVAILABLE and self.documents:
            print("üîç –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞...")
            self.build_bm25_index()
            print("‚úÖ BM25 –∏–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤")
    
    def _load_manifest(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å manifest —Å —É—á–µ—Ç–æ–º —è–∑—ã–∫–∞"""
        if self.lang and self.language_router:
            # –Ø–∑—ã–∫ –∑–∞–¥–∞–Ω - –∑–∞–≥—Ä—É–∂–∞–µ–º —è–∑—ã–∫–æ–≤–æ–π manifest
            manifest_path = self.language_router.get_manifest_path(self.lang)
        else:
            # –Ø–∑—ã–∫ –Ω–µ –∑–∞–¥–∞–Ω - –∑–∞–≥—Ä—É–∂–∞–µ–º –æ–±—â–∏–π manifest (legacy)
            manifest_path = self.knowledge_dir / 'manifest.json'
        
        if manifest_path.exists():
            try:
                with open(manifest_path, 'r', encoding='utf-8') as f:
                    manifest = json.load(f)
                    self.kb_version = manifest.get('version', 'unknown')
                    print(f"üì¶ –í–µ—Ä—Å–∏—è –±–∞–∑—ã: {self.kb_version}")
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ manifest: {e}")
                self.kb_version = 'unknown'
        else:
            self.kb_version = 'unknown'
    
    def _get_search_directories(self) -> List[Path]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if self.lang and self.language_router:
            # –Ø–∑—ã–∫ –∑–∞–¥–∞–Ω - –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–∑ —è–∑—ã–∫–æ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            lang_dir = self.language_router.get_docs_dir(self.lang)
            if lang_dir.exists():
                return [lang_dir]
            else:
                print(f"‚ö†Ô∏è  –Ø–∑—ã–∫–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {lang_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return []
        else:
            # –Ø–∑—ã–∫ –Ω–µ –∑–∞–¥–∞–Ω - –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –∫–æ—Ä–Ω—è (legacy) –∏ –∏–∑ –æ–±–µ–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –ø–∞–ø–æ–∫ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            dirs = [self.knowledge_dir]
            if self.language_router:
                for lang in ["rus", "eng"]:
                    lang_dir = self.language_router.get_docs_dir(lang)
                    if lang_dir.exists():
                        dirs.append(lang_dir)
            return dirs
    
    def extract_metadata_and_content(self, content: str, file_path: Path) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç"""
        try:
            # –†–∞–∑–¥–µ–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —Ç–µ–ª–æ
            if content.startswith('---'):
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    metadata_str = parts[1]
                    body = parts[2].strip()
                else:
                    return None
            else:
                return None
            
            # –ü–∞—Ä—Å–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            doc = {'body': body, 'file_path': str(file_path)}
            
            for line in metadata_str.split('\n'):
                line = line.strip()
                if ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip().strip('"').strip("'")
                    
                    if key in ['title', 'summary', 'category', 'subcategory', 'lang']:
                        doc[key] = value
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º tags (–º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏ —Ä—É—Å—Å–∫–∏–µ, –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã)
            tags_match = re.search(r'tags:\s*\[(.*?)\]', metadata_str)
            if tags_match:
                tags_str = tags_match.group(1)
                # –ü–∞—Ä—Å–∏–º —Å–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤
                tags = [t.strip().strip('"').strip("'") for t in tags_str.split(',')]
                doc['tags'] = tags
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º source —Ñ–∞–π–ª—ã
            source_match = re.search(r'- path:\s*"([^"]+)"', metadata_str)
            if source_match:
                doc['source_file'] = source_match.group(1).replace('raw/', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–∞–π–¥—ã
            slides_match = re.search(r'slides:\s*\[(\d+)-(\d+)\]', metadata_str)
            if slides_match:
                doc['slides_start'] = slides_match.group(1)
                doc['slides_end'] = slides_match.group(2)
            
            return doc
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {file_path.name}: {e}")
            return None
    
    def _get_multilingual_terms(self, title: str, category: str, subcategory: str) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        terms = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –∏–∑ title, category, subcategory
        all_text = f"{title} {category} {subcategory}".lower()
        words = re.findall(r'\w+', all_text)
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –∏—â–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã –≤ —Å–ª–æ–≤–∞—Ä–µ
        for word in words:
            if len(word) < 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                continue
            
            # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ä–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤
            for english_key, variants in self.normalizer.translations.items():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ —Å–ª–æ–≤–æ —Å –∫–∞–∫–∏–º-—Ç–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–º
                for variant in variants:
                    variant_clean = variant.replace(' ', '').lower()
                    if word.startswith(variant_clean[:min(5, len(variant_clean))]):
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —ç—Ç–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
                        terms.extend(variants)
                        break
        
        return list(set(terms))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def build_bm25_index(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å BM25 –∏–Ω–¥–µ–∫—Å"""
        if not BM25_AVAILABLE:
            print("‚ùå BM25 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        self.tokenized_corpus = []
        for doc in self.documents:
            # –í–ê–ñ–ù–û: –ü–æ–≤—Ç–æ—Ä—è–µ–º title –∏ category 3 —Ä–∞–∑–∞ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∏—Ö –≤–µ—Å–∞
            # –≠—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –¥–ª—è boosting –≤–∞–∂–Ω—ã—Ö –ø–æ–ª–µ–π
            title = doc.get('title', '')
            category = doc.get('category', '')
            subcategory = doc.get('subcategory', '')
            tags = doc.get('tags', [])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è title –∏ category
            # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–∫–∞—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º
            multilingual_terms = self._get_multilingual_terms(title, category, subcategory)
            
            text_parts = [
                title, title, title,  # 3x boost –¥–ª—è title
                category, category,   # 2x boost –¥–ª—è category
                subcategory,
                ' '.join(tags),       # –î–æ–±–∞–≤–ª—è–µ–º tags (—Å–æ–¥–µ—Ä–∂–∞—Ç –æ–±–∞ —è–∑—ã–∫–∞)
                ' '.join(multilingual_terms),  # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã/–ø–µ—Ä–µ–≤–æ–¥—ã
                doc.get('summary', ''),
                doc.get('body', '')[:5000]  # –ü–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–ª–∞
            ]
            
            full_text = ' '.join(text_parts)
            tokens = self.normalizer.tokenize(full_text)
            self.tokenized_corpus.append(tokens)
        
        # –°–æ–∑–¥–∞—ë–º BM25 –∏–Ω–¥–µ–∫—Å
        self.bm25 = BM25Okapi(self.tokenized_corpus)
    
    def search_documents(self, query: str, limit: int = 5) -> List[Tuple[Dict, float]]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å BM25"""
        if not BM25_AVAILABLE or not self.bm25:
            print("‚ùå BM25 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
            return self._fallback_search(query, limit)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_tokens = self.normalizer.tokenize(query)
        query_normalized = ' '.join(query_tokens)
        
        # BM25 —Å–∫–æ—Ä–∏–Ω–≥
        scores = self.bm25.get_scores(query_tokens)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        results = []
        for idx, score in enumerate(scores):
            if score > 0:
                doc = self.documents[idx]
                final_score = float(score)
                
                # –ë–û–ù–£–°: —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–∞–∂–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ title/category
                title = doc.get('title', '')
                category = doc.get('category', '')
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤–º–µ—Å—Ç–µ (–≤–∞–∂–Ω–æ –¥–ª—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–π —Ç–∏–ø–∞ "—Å—Ç—Ä–∞–Ω–∞ + –ø—Ä–æ–≥—Ä–∞–º–º–∞")
                combined_text = f"{category} {title}"
                combined_tokens = self.normalizer.tokenize(combined_text)
                
                # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –µ—Å—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
                significant_tokens = [t for t in query_tokens if len(t) > 3]
                matches = sum(1 for token in significant_tokens if token in combined_tokens)
                
                # –£—Å–∏–ª–µ–Ω–Ω—ã–π –±—É—Å—Ç –µ—Å–ª–∏ —Å–æ–≤–ø–∞–ª–∏ –í–°–ï –∏–ª–∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∑–Ω–∞—á–∏–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
                if len(significant_tokens) > 0:
                    match_ratio = matches / len(significant_tokens)
                    
                    if match_ratio >= 0.8:  # 80%+ —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ–≤–ø–∞–ª–æ
                        final_score *= 3.0  # –°–∏–ª—å–Ω—ã–π –±—É—Å—Ç
                    elif match_ratio >= 0.5:  # 50%+ —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ–≤–ø–∞–ª–æ
                        final_score *= 2.0  # –°—Ä–µ–¥–Ω–∏–π –±—É—Å—Ç
                    elif match_ratio > 0:  # –•–æ—Ç—å —á—Ç–æ-—Ç–æ —Å–æ–≤–ø–∞–ª–æ
                        final_score *= (1 + match_ratio * 0.5)  # –ù–µ–±–æ–ª—å—à–æ–π –±—É—Å—Ç
                
                results.append((doc, final_score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results[:limit]
    
    def _fallback_search(self, query: str, limit: int = 5) -> List[Tuple[Dict, float]]:
        """–ü—Ä–æ—Å—Ç–æ–π fallback –ø–æ–∏—Å–∫ –µ—Å–ª–∏ BM25 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        query_lower = query.lower()
        query_words = set(query_lower.split())
        
        results = []
        for doc in self.documents:
            body_lower = doc.get('body', '').lower()
            title_lower = doc.get('title', '').lower()
            
            score = 0
            for word in query_words:
                if len(word) > 2:
                    if word in title_lower:
                        score += 10
                    if word in body_lower:
                        score += 1
            
            if score > 0:
                results.append((doc, score))
        
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:limit]
    
    def extract_relevant_content(self, doc: Dict, query: str, context_size: int = 6000) -> str:
        """–ò–∑–≤–ª–µ—á—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        body = doc.get('body', '')
        title = doc.get('title', '')
        summary = doc.get('summary', '')
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = f"# {title}\n\n"
        if summary:
            context += f"**–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:** {summary}\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–µ—Ä–≤—ã–µ —Å–ª–∞–π–¥—ã)
        if body:
            slides = body.split('--- –°–ª–∞–π–¥')
            relevant_slides = []
            for i, slide in enumerate(slides[:15]):
                if slide.strip():
                    if i > 0:
                        relevant_slides.append('--- –°–ª–∞–π–¥' + slide)
                    else:
                        relevant_slides.append(slide)
            
            body_excerpt = '\n'.join(relevant_slides)
            if len(body_excerpt) > context_size:
                body_excerpt = body_excerpt[:context_size] + "\n\n[...–¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è...]"
            
            context += body_excerpt
        
        return context
    
    def generate_llm_answer(self, query: str, context: str, sources: List[str]) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é LLM —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π rate limits"""
        if not self.groq_client:
            return None
        
        import time
        import requests
        
        max_retries = 3
        base_delay = 2
        
        for attempt in range(max_retries):
            try:
                system_prompt = """–¢—ã - –æ–ø—ã—Ç–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∞–º –∏–º–º–∏–≥—Ä–∞—Ü–∏–∏ –∏ –ø–æ–ª—É—á–µ–Ω–∏—è –≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–∞.

–°–¢–ò–õ–¨ –û–ë–©–ï–ù–ò–Ø:
- –û—Ç–≤–µ—á–∞–π –∫–∞–∫ –∂–∏–≤–æ–π —á–µ–ª–æ–≤–µ–∫, –±–µ–∑ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö "–í–≤–µ–¥–µ–Ω–∏–µ" –∏ "–ó–∞–∫–ª—é—á–µ–Ω–∏–µ"
- –°—Ä–∞–∑—É –ø–µ—Ä–µ—Ö–æ–¥–∏ –∫ —Å—É—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º
- –ò—Å–ø–æ–ª—å–∑—É–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π —è–∑—ã–∫

–§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û):
- –ò—Å–ø–æ–ª—å–∑—É–π HTML –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (—ç—Ç–æ –¥–ª—è Telegram)
- <b>–ñ–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç</b> –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤, –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–æ–≥—Ä–∞–º–º, —Å—Ç—Ä–∞–Ω
- <i>–ö—É—Ä—Å–∏–≤</i> –¥–ª—è –ø–æ—è—Å–Ω–µ–Ω–∏–π –∏ –ø—Ä–∏–º–µ—á–∞–Ω–∏–π
- –°–ø–∏—Å–∫–∏ —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏ (‚Ä¢) –∏–ª–∏ –Ω–æ–º–µ—Ä–∞–º–∏ –¥–ª—è –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–π
- –†–∞–∑–±–∏–≤–∞–π –¥–ª–∏–Ω–Ω—ã–µ –∞–±–∑–∞—Ü—ã –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –º–∞–∫—Å)
- –°—É–º–º—ã –∏ —Å—Ä–æ–∫–∏ –≤—ã–¥–µ–ª—è–π –∂–∏—Ä–Ω—ã–º: <b>$250,000</b>, <b>3-4 –º–µ—Å—è—Ü–∞</b>
- –ò—Å–ø–æ–ª—å–∑—É–π \n\n –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –∞–±–∑–∞—Ü–µ–≤ (–¥–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏)
- –≠–º–æ–¥–∑–∏ –∏—Å–ø–æ–ª—å–∑—É–π —É–º–µ—Ä–µ–Ω–Ω–æ, —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–∞–∂–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤ (‚úì, ‚Ä¢, üí∞, üìÖ, üåç)
- –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π <h1>, <h2>, <h3> - —Ç–æ–ª—å–∫–æ <b> –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
- –ò—Å–ø–æ–ª—å–∑—É–π —Å–∏–º–≤–æ–ª—ã –¥–ª—è –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: üîπ <b>–ó–∞–≥–æ–ª–æ–≤–æ–∫</b>

–ü–†–ê–í–ò–õ–ê:
1. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç - —Å–∫–∞–∂–∏ "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —Ç–∞–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –Ω–∞—à–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö"
3. –í—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, —Å—É–º–º—ã, —Å—Ä–æ–∫–∏
4. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç –ª–æ–≥–∏—á–Ω–æ –∏ –∫—Ä–∞—Å–∏–≤–æ
5. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, —á—Ç–æ –∏ –≤–æ–ø—Ä–æ—Å
6. –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π - —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""

                user_prompt = f"""–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞:
{query}

–î–æ—Å—Ç—É–ø–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
{context}

–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞ –∫—Ä–∞—Å–∏–≤–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ. –ò—Å–ø–æ–ª—å–∑—É–π HTML-—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: <b>–∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç</b>, <i>–∫—É—Ä—Å–∏–≤</i>, —Å–ø–∏—Å–∫–∏ —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏. –†–∞–∑–±–∏–≤–∞–π —Ç–µ–∫—Å—Ç –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã (–∏—Å–ø–æ–ª—å–∑—É–π \\n\\n –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è)."""

                response = self.groq_client.chat.completions.create(
                    model="llama-3.3-70b-versatile",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.3,
                    max_tokens=1500,
                )
                
                llm_answer = response.choices[0].message.content
                
                final_answer = llm_answer + "\n\n---\n\n**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n"
                for source in sources:
                    final_answer += f"- {source}\n"
                
                return final_answer
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 429:
                    # Rate limit exceeded
                    delay = base_delay * (2 ** attempt)  # Exponential backoff
                    print(f"‚ö†Ô∏è  Rate limit exceeded (429). –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}, –æ–∂–∏–¥–∞–Ω–∏–µ {delay} —Å–µ–∫...")
                    
                    if attempt < max_retries - 1:
                        time.sleep(delay)
                        continue
                    else:
                        # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º fallback –æ—Ç–≤–µ—Ç
                        print(f"‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã. –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback –æ—Ç–≤–µ—Ç.")
                        return self._generate_fallback_answer(query, context, sources)
                else:
                    print(f"‚ö†Ô∏è  HTTP –æ—à–∏–±–∫–∞ LLM: {e}")
                    return None
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ LLM: {e}")
                if attempt < max_retries - 1:
                    delay = base_delay * (2 ** attempt)
                    print(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {delay} —Å–µ–∫...")
                    time.sleep(delay)
                else:
                    return None
        
        return None
    
    def _generate_fallback_answer(self, query: str, context: str, sources: List[str]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å fallback –æ—Ç–≤–µ—Ç –∫–æ–≥–¥–∞ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        # –ü—Ä–æ—Å—Ç–æ–π fallback –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        fallback = f"""<b>üìö –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π</b>

–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç —Å–µ—Ä–≤–∏—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω. 
–ù–æ —è –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É:

<b>üîç –ù–∞–π–¥–µ–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã:</b>
"""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        lines = context.split('\n')
        titles = []
        for line in lines:
            if line.startswith('# ') and len(line) > 3:
                titles.append(line[2:].strip())
        
        if titles:
            for title in titles[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ 5 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                fallback += f"‚Ä¢ {title}\n"
        else:
            fallback += "‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞–π–¥–µ–Ω—ã –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π\n"
        
        fallback += f"""

<b>‚è≥ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ 1-2 –º–∏–Ω—É—Ç—ã</b> - —Å–µ—Ä–≤–∏—Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.

<b>üìã –ò—Å—Ç–æ—á–Ω–∏–∫–∏:</b>
"""
        
        for source in sources[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ 3 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            fallback += f"‚Ä¢ {source}\n"
        
        return fallback
    
    def format_answer(self, query: str, results: List[Tuple[Dict, float]]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"""
        if not results:
            return "‚ùå –ù–µ –∑–Ω–∞—é ‚Äî –Ω–µ—Ç –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö.\n\n–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        contexts = []
        sources = []
        
        for doc, score in results[:3]:
            context = self.extract_relevant_content(doc, query)
            contexts.append(context)
            
            source_file = doc.get('source_file', 'Unknown')
            slides_start = doc.get('slides_start', '?')
            slides_end = doc.get('slides_end', '?')
            source_line = f"raw/{source_file} ‚Üí —Å–ª–∞–π–¥—ã {slides_start}‚Äì{slides_end}"
            if source_line not in sources:
                sources.append(source_line)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LLM
        if self.groq_client and contexts:
            full_context = "\n\n---\n\n".join(contexts)
            llm_answer = self.generate_llm_answer(query, full_context, sources)
            if llm_answer:
                return llm_answer
        
        # Fallback –µ—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        answer = contexts[0]
        answer += "\n\n---\n\n**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n"
        for source in sources:
            answer += f"- {source}\n"
        
        return answer
    
    def ask(self, question: str, lang: Optional[Language] = None) -> str:
        """
        –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —è–∑—ã–∫–∞
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            lang: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ —è–∑—ã–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            –û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –∑–∞–ø—Ä–æ—Å–∞
        detected_lang = None
        if self.auto_detect_lang and self.language_detector and not lang:
            detected_lang = self.language_detector.detect_from_query(question)
            
            # –ï—Å–ª–∏ –∞–≥–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —è–∑—ã–∫–æ–º –∏ –æ–Ω –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è - –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º
            if self.lang and detected_lang != self.lang:
                print(f"‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ {detected_lang}, –Ω–æ –∞–≥–µ–Ω—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç —Å {self.lang}")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —è–∑—ã–∫ –∞–≥–µ–Ω—Ç–∞
                detected_lang = self.lang
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π lang –∏–ª–∏ —è–∑—ã–∫ –∞–≥–µ–Ω—Ç–∞
            detected_lang = lang or self.lang
        
        # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        results = self.search_documents(question, limit=5)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
        answer = self.format_answer(question, results)
        
        return answer
    
    def set_language(self, lang: Optional[Language]):
        """
        –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å —è–∑—ã–∫ –∞–≥–µ–Ω—Ç–∞ (–ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π)
        
        Args:
            lang: –ù–æ–≤—ã–π —è–∑—ã–∫ ("rus", "eng" –∏–ª–∏ None –¥–ª—è –æ–±–æ–∏—Ö)
        """
        if lang not in [None, "rus", "eng"]:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —è–∑—ã–∫: {lang}. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'rus', 'eng' –∏–ª–∏ None")
        
        self.lang = lang
        self.documents = []
        self.tokenized_corpus = []
        self.bm25 = None
        
        # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å –Ω–æ–≤—ã–º —è–∑—ã–∫–æ–º
        self.load_knowledge_base()
        
        lang_label = f"{lang.upper()}" if lang else "–≤—Å–µ —è–∑—ã–∫–∏"
        print(f"üåç –Ø–∑—ã–∫ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω –Ω–∞: {lang_label}")


# –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
KnowledgeAgent = KnowledgeAgentV3


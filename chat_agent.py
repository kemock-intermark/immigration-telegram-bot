#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å BM25 –ø–æ–∏—Å–∫–æ–º
–í–µ—Ä—Å–∏—è 3.0 - –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±–µ–∑ –∫–æ—Å—Ç—ã–ª–µ–π
"""

import json
import re
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
from functools import lru_cache

# BM25 –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
try:
    from rank_bm25 import BM25Okapi
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False
    print("‚ö†Ô∏è  rank_bm25 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install rank-bm25")

# LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
try:
    from groq import Groq
    import os
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    print("‚ö†Ô∏è  Groq –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")


class TextNormalizer:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞"""
    
    def __init__(self):
        # –î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –ø–µ—Ä–µ–≤–æ–¥–æ–≤: RU ‚Üî EN
        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–∫–∞—Ç—å –Ω–∞ –ª—é–±–æ–º —è–∑—ã–∫–µ
        self.translations = {
            # –°—Ç—Ä–∞–Ω—ã (–∫–æ—Ä–µ–Ω—å —Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ ‚Üí –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
            'portugal': ['–ø–æ—Ä—Ç—É–≥–∞–ª', 'portugal'],
            'greece': ['–≥—Ä–µ—Ü', '–≥—Ä–µ—á', '–≥—Ä–µ—á–µ—Å–∫', 'greece'],
            'turkey': ['—Ç—É—Ä—Ü', '—Ç—É—Ä–µ—Ü', '—Ç—É—Ä–µ—Ü–∫', 'turkey'],
            'grenada': ['–≥—Ä–µ–Ω–∞–¥', 'grenada'],
            'malta': ['–º–∞–ª—å—Ç', 'malta'],
            'cyprus': ['–∫–∏–ø—Ä', 'cyprus'],
            'spain': ['–∏—Å–ø–∞–Ω', 'spain'],
            'paraguay': ['–ø–∞—Ä–∞–≥–≤–∞', 'paraguay'],
            'vanuatu': ['–≤–∞–Ω—É–∞—Ç—É', 'vanuatu'],
            'dominica': ['–¥–æ–º–∏–Ω–∏–∫', 'dominica'],
            'antigua': ['–∞–Ω—Ç–∏–≥—É–∞', 'antigua'],
            'barbuda': ['–±–∞—Ä–±—É–¥', 'barbuda'],
            'caribbean': ['–∫–∞—Ä–∏–±—Å–∫', 'caribbean'],
            'saint kitts': ['—Å–µ–Ω—Ç –∫–∏—Ç—Ç—Å', '—Å–µ–Ω—Ç –∫–∏—Ç—Å', 'saint kitts', 'st kitts'],
            'saint lucia': ['—Å–µ–Ω—Ç –ª—é—Å–∏', '—Å–µ–Ω—Ç –ª—é—á–∏', 'saint lucia', 'st lucia'],
            'france': ['—Ñ—Ä–∞–Ω—Ü', 'france'],
            'germany': ['–≥–µ—Ä–º–∞–Ω', 'germany', 'deutschland'],
            'italy': ['–∏—Ç–∞–ª', 'italy'],
            'austria': ['–∞–≤—Å—Ç—Ä', 'austria'],
            'hungary': ['–≤–µ–Ω–≥—Ä', 'hungary'],
            'bulgaria': ['–±–æ–ª–≥–∞—Ä', 'bulgaria'],
            'serbia': ['—Å–µ—Ä–±', 'serbia'],
            'montenegro': ['—á–µ—Ä–Ω–æ–≥–æ—Ä', 'montenegro'],
            'latvia': ['–ª–∞—Ç–≤', 'latvia'],
            'slovenia': ['—Å–ª–æ–≤–µ–Ω', 'slovenia'],
            'luxembourg': ['–ª—é–∫—Å–µ–º–±—É—Ä–≥', 'luxembourg'],
            'canada': ['–∫–∞–Ω–∞–¥', 'canada'],
            'usa': ['—Å—à–∞', 'usa', '–∞–º–µ—Ä–∏–∫'],
            'uk': ['–≤–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω', '–±—Ä–∏—Ç–∞–Ω', 'uk', 'united kingdom'],
            'egypt': ['–µ–≥–∏–ø—Ç', 'egypt'],
            'nauru': ['–Ω–∞—É—Ä—É', 'nauru'],
            'sao tome': ['—Å–∞–Ω —Ç–æ–º', 'sao tome'],
            
            # –ü—Ä–æ–≥—Ä–∞–º–º—ã –∏–º–º–∏–≥—Ä–∞—Ü–∏–∏
            'citizenship': ['–≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤', 'citizenship', '–ø–∞—Å–ø–æ—Ä—Ç', 'passport'],
            'residence permit': ['–≤–Ω–∂', '–≤–∏–¥ –Ω–∞ –∂–∏—Ç–µ–ª—å—Å—Ç–≤', 'residence permit', '–≤–∏–∑', 'visa'],
            'permanent residence': ['–ø–º–∂', '–ø–æ—Å—Ç–æ—è–Ω–Ω –ø—Ä–æ–∂–∏–≤–∞–Ω', 'permanent residence'],
            'golden visa': ['–∑–æ–ª–æ—Ç –≤–∏–∑', '–≥–æ–ª–¥–µ–Ω –≤–∏–∑', 'golden visa'],
            'investment': ['–∏–Ω–≤–µ—Å—Ç–∏—Ü', 'investment'],
            'digital nomad': ['—Ü–∏—Ñ—Ä–æ–≤ –∫–æ—á–µ–≤–Ω–∏–∫', 'digital nomad'],
            'startup': ['—Å—Ç–∞—Ä—Ç–∞–ø', 'startup'],
            'business': ['–±–∏–∑–Ω–µ—Å', 'business'],
            
            # –û–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
            'cost': ['—Å—Ç–æ–∏–º–æ—Å—Ç', '—Ü–µ–Ω', 'cost', 'price'],
            'requirements': ['—Ç—Ä–µ–±–æ–≤–∞–Ω', '—É—Å–ª–æ–≤', 'requirements', 'conditions'],
            'timeline': ['—Å—Ä–æ–∫', 'timeline'],
        }
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∑–∞–º–µ–Ω—ã (—Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞, –Ω–æ —Ç–µ–ø–µ—Ä—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        self.replacements = {}
        for english_key, variants in self.translations.items():
            for variant in variants:
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω —Å –æ–∫–æ–Ω—á–∞–Ω–∏—è–º–∏
                pattern = variant.replace(' ', r'\s+') + r'\w*'
                self.replacements[pattern] = english_key.replace(' ', '_')
        
    def normalize(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç"""
        text = text.lower()
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–º–µ–Ω—ã
        for pattern, replacement in self.replacements.items():
            text = re.sub(pattern, replacement, text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def tokenize(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏—Ç—å –Ω–∞ —Ç–æ–∫–µ–Ω—ã"""
        normalized = self.normalize(text)
        tokens = normalized.split()
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (—Å—Ç–æ–ø-—Å–ª–æ–≤–∞)
        tokens = [t for t in tokens if len(t) > 2]
        return tokens


class KnowledgeAgentV3:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç —Å BM25 –ø–æ–∏—Å–∫–æ–º"""
    
    def __init__(self, knowledge_dir: str):
        self.knowledge_dir = Path(knowledge_dir)
        self.documents = []
        self.normalizer = TextNormalizer()
        self.bm25 = None
        self.tokenized_corpus = []
        self.kb_version = None
        
        # LLM –∫–ª–∏–µ–Ω—Ç
        self.groq_client = None
        if GROQ_AVAILABLE:
            groq_api_key = os.getenv('GROQ_API_KEY')
            if groq_api_key:
                try:
                    self.groq_client = Groq(api_key=groq_api_key)
                except Exception as e:
                    print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Groq: {e}")
        
        self.load_knowledge_base()
    
    def load_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Ä—Å–∏—é –∏–∑ manifest –µ—Å–ª–∏ –µ—Å—Ç—å
        manifest_path = self.knowledge_dir / 'manifest.json'
        if manifest_path.exists():
            try:
                with open(manifest_path, 'r', encoding='utf-8') as f:
                    manifest = json.load(f)
                    self.kb_version = manifest.get('version', 'unknown')
                    print(f"üì¶ –í–µ—Ä—Å–∏—è –±–∞–∑—ã: {self.kb_version}")
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ manifest: {e}")
                self.kb_version = 'unknown'
        else:
            self.kb_version = 'unknown'
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ .md —Ñ–∞–π–ª—ã –∫—Ä–æ–º–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö
        md_files = []
        for md_file in self.knowledge_dir.rglob("*.md"):
            if not md_file.name.startswith(('00_', '_')):
                md_files.append(md_file)
        
        for md_file in md_files:
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                doc = self.extract_metadata_and_content(content, md_file)
                if doc:
                    self.documents.append(doc)
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {md_file.name}: {e}")
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –°—Ç—Ä–æ–∏–º BM25 –∏–Ω–¥–µ–∫—Å
        if BM25_AVAILABLE and self.documents:
            print("üîç –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞...")
            self.build_bm25_index()
            print("‚úÖ BM25 –∏–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤")
    
    def extract_metadata_and_content(self, content: str, file_path: Path) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç"""
        try:
            # –†–∞–∑–¥–µ–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —Ç–µ–ª–æ
            if content.startswith('---'):
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    metadata_str = parts[1]
                    body = parts[2].strip()
                else:
                    return None
            else:
                return None
            
            # –ü–∞—Ä—Å–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            doc = {'body': body, 'file_path': str(file_path)}
            
            for line in metadata_str.split('\n'):
                line = line.strip()
                if ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip().strip('"').strip("'")
                    
                    if key in ['title', 'summary', 'category', 'subcategory']:
                        doc[key] = value
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º tags (–º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏ —Ä—É—Å—Å–∫–∏–µ, –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã)
            tags_match = re.search(r'tags:\s*\[(.*?)\]', metadata_str)
            if tags_match:
                tags_str = tags_match.group(1)
                # –ü–∞—Ä—Å–∏–º —Å–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤
                tags = [t.strip().strip('"').strip("'") for t in tags_str.split(',')]
                doc['tags'] = tags
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º source —Ñ–∞–π–ª—ã
            source_match = re.search(r'- path:\s*"([^"]+)"', metadata_str)
            if source_match:
                doc['source_file'] = source_match.group(1).replace('raw/', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–∞–π–¥—ã
            slides_match = re.search(r'slides:\s*\[(\d+)-(\d+)\]', metadata_str)
            if slides_match:
                doc['slides_start'] = slides_match.group(1)
                doc['slides_end'] = slides_match.group(2)
            
            return doc
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {file_path.name}: {e}")
            return None
    
    def _get_multilingual_terms(self, title: str, category: str, subcategory: str) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        terms = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –∏–∑ title, category, subcategory
        all_text = f"{title} {category} {subcategory}".lower()
        words = re.findall(r'\w+', all_text)
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –∏—â–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã –≤ —Å–ª–æ–≤–∞—Ä–µ
        for word in words:
            if len(word) < 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                continue
            
            # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ä–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤
            for english_key, variants in self.normalizer.translations.items():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ —Å–ª–æ–≤–æ —Å –∫–∞–∫–∏–º-—Ç–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–º
                for variant in variants:
                    variant_clean = variant.replace(' ', '').lower()
                    if word.startswith(variant_clean[:min(5, len(variant_clean))]):
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —ç—Ç–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
                        terms.extend(variants)
                        break
        
        return list(set(terms))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def build_bm25_index(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å BM25 –∏–Ω–¥–µ–∫—Å"""
        if not BM25_AVAILABLE:
            print("‚ùå BM25 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        self.tokenized_corpus = []
        for doc in self.documents:
            # –í–ê–ñ–ù–û: –ü–æ–≤—Ç–æ—Ä—è–µ–º title –∏ category 3 —Ä–∞–∑–∞ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∏—Ö –≤–µ—Å–∞
            # –≠—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –¥–ª—è boosting –≤–∞–∂–Ω—ã—Ö –ø–æ–ª–µ–π
            title = doc.get('title', '')
            category = doc.get('category', '')
            subcategory = doc.get('subcategory', '')
            tags = doc.get('tags', [])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è title –∏ category
            # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–∫–∞—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º
            multilingual_terms = self._get_multilingual_terms(title, category, subcategory)
            
            text_parts = [
                title, title, title,  # 3x boost –¥–ª—è title
                category, category,   # 2x boost –¥–ª—è category
                subcategory,
                ' '.join(tags),       # –î–æ–±–∞–≤–ª—è–µ–º tags (—Å–æ–¥–µ—Ä–∂–∞—Ç –æ–±–∞ —è–∑—ã–∫–∞)
                ' '.join(multilingual_terms),  # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã/–ø–µ—Ä–µ–≤–æ–¥—ã
                doc.get('summary', ''),
                doc.get('body', '')[:5000]  # –ü–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–ª–∞
            ]
            
            full_text = ' '.join(text_parts)
            tokens = self.normalizer.tokenize(full_text)
            self.tokenized_corpus.append(tokens)
        
        # –°–æ–∑–¥–∞—ë–º BM25 –∏–Ω–¥–µ–∫—Å
        self.bm25 = BM25Okapi(self.tokenized_corpus)
    
    def search_documents(self, query: str, limit: int = 5) -> List[Tuple[Dict, float]]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å BM25"""
        if not BM25_AVAILABLE or not self.bm25:
            print("‚ùå BM25 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
            return self._fallback_search(query, limit)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_tokens = self.normalizer.tokenize(query)
        query_normalized = ' '.join(query_tokens)
        
        # BM25 —Å–∫–æ—Ä–∏–Ω–≥
        scores = self.bm25.get_scores(query_tokens)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        results = []
        for idx, score in enumerate(scores):
            if score > 0:
                doc = self.documents[idx]
                final_score = float(score)
                
                # –ë–û–ù–£–°: —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–∞–∂–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ title/category
                title = doc.get('title', '')
                category = doc.get('category', '')
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤–º–µ—Å—Ç–µ (–≤–∞–∂–Ω–æ –¥–ª—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–π —Ç–∏–ø–∞ "—Å—Ç—Ä–∞–Ω–∞ + –ø—Ä–æ–≥—Ä–∞–º–º–∞")
                combined_text = f"{category} {title}"
                combined_tokens = self.normalizer.tokenize(combined_text)
                
                # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –µ—Å—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
                significant_tokens = [t for t in query_tokens if len(t) > 3]
                matches = sum(1 for token in significant_tokens if token in combined_tokens)
                
                # –£—Å–∏–ª–µ–Ω–Ω—ã–π –±—É—Å—Ç –µ—Å–ª–∏ —Å–æ–≤–ø–∞–ª–∏ –í–°–ï –∏–ª–∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∑–Ω–∞—á–∏–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
                if len(significant_tokens) > 0:
                    match_ratio = matches / len(significant_tokens)
                    
                    if match_ratio >= 0.8:  # 80%+ —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ–≤–ø–∞–ª–æ
                        final_score *= 3.0  # –°–∏–ª—å–Ω—ã–π –±—É—Å—Ç
                    elif match_ratio >= 0.5:  # 50%+ —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ–≤–ø–∞–ª–æ
                        final_score *= 2.0  # –°—Ä–µ–¥–Ω–∏–π –±—É—Å—Ç
                    elif match_ratio > 0:  # –•–æ—Ç—å —á—Ç–æ-—Ç–æ —Å–æ–≤–ø–∞–ª–æ
                        final_score *= (1 + match_ratio * 0.5)  # –ù–µ–±–æ–ª—å—à–æ–π –±—É—Å—Ç
                
                results.append((doc, final_score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results[:limit]
    
    def _fallback_search(self, query: str, limit: int = 5) -> List[Tuple[Dict, float]]:
        """–ü—Ä–æ—Å—Ç–æ–π fallback –ø–æ–∏—Å–∫ –µ—Å–ª–∏ BM25 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        query_lower = query.lower()
        query_words = set(query_lower.split())
        
        results = []
        for doc in self.documents:
            body_lower = doc.get('body', '').lower()
            title_lower = doc.get('title', '').lower()
            
            score = 0
            for word in query_words:
                if len(word) > 2:
                    if word in title_lower:
                        score += 10
                    if word in body_lower:
                        score += 1
            
            if score > 0:
                results.append((doc, score))
        
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:limit]
    
    def extract_relevant_content(self, doc: Dict, query: str, context_size: int = 6000) -> str:
        """–ò–∑–≤–ª–µ—á—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        body = doc.get('body', '')
        title = doc.get('title', '')
        summary = doc.get('summary', '')
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = f"# {title}\n\n"
        if summary:
            context += f"**–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:** {summary}\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–µ—Ä–≤—ã–µ —Å–ª–∞–π–¥—ã)
        if body:
            slides = body.split('--- –°–ª–∞–π–¥')
            relevant_slides = []
            for i, slide in enumerate(slides[:15]):
                if slide.strip():
                    if i > 0:
                        relevant_slides.append('--- –°–ª–∞–π–¥' + slide)
                    else:
                        relevant_slides.append(slide)
            
            body_excerpt = '\n'.join(relevant_slides)
            if len(body_excerpt) > context_size:
                body_excerpt = body_excerpt[:context_size] + "\n\n[...–¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è...]"
            
            context += body_excerpt
        
        return context
    
    def generate_llm_answer(self, query: str, context: str, sources: List[str]) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é LLM"""
        if not self.groq_client:
            return None
        
        try:
            system_prompt = """–¢—ã - –æ–ø—ã—Ç–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∞–º –∏–º–º–∏–≥—Ä–∞—Ü–∏–∏ –∏ –ø–æ–ª—É—á–µ–Ω–∏—è –≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–∞.

–°–¢–ò–õ–¨ –û–ë–©–ï–ù–ò–Ø:
- –û—Ç–≤–µ—á–∞–π –∫–∞–∫ –∂–∏–≤–æ–π —á–µ–ª–æ–≤–µ–∫, –±–µ–∑ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö "–í–≤–µ–¥–µ–Ω–∏–µ" –∏ "–ó–∞–∫–ª—é—á–µ–Ω–∏–µ"
- –°—Ä–∞–∑—É –ø–µ—Ä–µ—Ö–æ–¥–∏ –∫ —Å—É—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º
- –ò—Å–ø–æ–ª—å–∑—É–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π —è–∑—ã–∫

–ü–†–ê–í–ò–õ–ê:
1. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç - —Å–∫–∞–∂–∏ "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —Ç–∞–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –Ω–∞—à–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö"
3. –í—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, —Å—É–º–º—ã, —Å—Ä–æ–∫–∏
4. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç (—Å–ø–∏—Å–∫–∏, –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∏) —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ —É–ª—É—á—à–∞–µ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ
5. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, —á—Ç–æ –∏ –≤–æ–ø—Ä–æ—Å
6. –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π - —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""

            user_prompt = f"""–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞:
{query}

–î–æ—Å—Ç—É–ø–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
{context}

–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –û—Ç–≤–µ—á–∞–π –ø–æ –¥–µ–ª—É, –±–µ–∑ –ª–∏—à–Ω–∏—Ö –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π."""

            response = self.groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=1500,
            )
            
            llm_answer = response.choices[0].message.content
            
            final_answer = llm_answer + "\n\n---\n\n**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n"
            for source in sources:
                final_answer += f"- {source}\n"
            
            return final_answer
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ LLM: {e}")
            return None
    
    def format_answer(self, query: str, results: List[Tuple[Dict, float]]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"""
        if not results:
            return "‚ùå –ù–µ –∑–Ω–∞—é ‚Äî –Ω–µ—Ç –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö.\n\n–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        contexts = []
        sources = []
        
        for doc, score in results[:3]:
            context = self.extract_relevant_content(doc, query)
            contexts.append(context)
            
            source_file = doc.get('source_file', 'Unknown')
            slides_start = doc.get('slides_start', '?')
            slides_end = doc.get('slides_end', '?')
            source_line = f"raw/{source_file} ‚Üí —Å–ª–∞–π–¥—ã {slides_start}‚Äì{slides_end}"
            if source_line not in sources:
                sources.append(source_line)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LLM
        if self.groq_client and contexts:
            full_context = "\n\n---\n\n".join(contexts)
            llm_answer = self.generate_llm_answer(query, full_context, sources)
            if llm_answer:
                return llm_answer
        
        # Fallback –µ—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        answer = contexts[0]
        answer += "\n\n---\n\n**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n"
        for source in sources:
            answer += f"- {source}\n"
        
        return answer
    
    def ask(self, question: str) -> str:
        """–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å"""
        results = self.search_documents(question, limit=5)
        answer = self.format_answer(question, results)
        return answer


# –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
KnowledgeAgent = KnowledgeAgentV3


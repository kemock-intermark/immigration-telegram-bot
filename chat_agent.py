#!/usr/bin/env python3
"""
–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç-–∞–≥–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏–∑ knowledge/
"""

import sys
import re
import hashlib
import json
import os
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
from datetime import datetime
from functools import lru_cache

# LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    print("‚ö†Ô∏è  Groq –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –û—Ç–≤–µ—Ç—ã –±—É–¥—É—Ç –±–µ–∑ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏.")


class KnowledgeAgent:
    def __init__(self, knowledge_dir: str):
        self.knowledge_dir = Path(knowledge_dir)
        self.documents = []
        self.conversation_history = []
        self.unanswered_log_path = self.knowledge_dir / "_unanswered_log.md"
        self.search_cache = {}  # –ü—Ä–æ—Å—Ç–æ–π –∫—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        self.kb_version = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Groq –∫–ª–∏–µ–Ω—Ç–∞
        self.groq_client = None
        if GROQ_AVAILABLE:
            groq_api_key = os.getenv('GROQ_API_KEY')
            if groq_api_key:
                try:
                    self.groq_client = Groq(api_key=groq_api_key)
                except Exception as e:
                    print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Groq: {e}")
        
        self.load_knowledge_base()
        
    def extract_metadata_and_content(self, md_path: Path) -> Dict:
        """–ò–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑ Markdown —Ñ–∞–π–ª–∞"""
        try:
            with open(md_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ò—â–µ–º YAML frontmatter –∏ —Ç–µ–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)$', content, re.DOTALL)
            if not match:
                return {}
            
            yaml_content = match.group(1)
            body_content = match.group(2)
            
            # –ü–∞—Ä—Å–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'body': body_content,
                'file_path': md_path,
                'file_name': md_path.name
            }
            
            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ YAML
            lines = yaml_content.split('\n')
            i = 0
            while i < len(lines):
                line = lines[i]
                if ':' in line and not line.strip().startswith('-'):
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip().strip('"\'')
                    
                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ source_files
                    if key == 'source_files' and i + 1 < len(lines):
                        # –ò—â–µ–º path –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö
                        j = i + 1
                        while j < len(lines) and (lines[j].startswith(' ') or lines[j].startswith('\t')):
                            subline = lines[j].strip()
                            if subline.startswith('- path:'):
                                path_match = re.search(r'path:\s*["\']?([^"\']+)["\']?', subline)
                                if path_match:
                                    full_path = path_match.group(1)
                                    metadata['source_file'] = full_path.replace('raw/', '')
                            elif subline.startswith('slides:'):
                                slides_match = re.search(r'slides:\s*\[(\d+)-(\d+)\]', subline)
                                if slides_match:
                                    metadata['slides_start'] = slides_match.group(1)
                                    metadata['slides_end'] = slides_match.group(2)
                            j += 1
                    else:
                        metadata[key] = value
                i += 1
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, –∏—â–µ–º –≤ —Ç–µ–ª–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if 'source_file' not in metadata:
                source_match = re.search(r'raw/([^\s‚Üí\]]+\.pdf)', body_content)
                if source_match:
                    metadata['source_file'] = source_match.group(1)
            
            if 'slides_start' not in metadata:
                slides_match = re.search(r'—Å–ª–∞–π–¥—ã?\s+(\d+)[-‚Äì](\d+)', body_content)
                if slides_match:
                    metadata['slides_start'] = slides_match.group(1)
                    metadata['slides_end'] = slides_match.group(2)
            
            return metadata
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {md_path.name}: {e}")
            return {}
    
    def load_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –≤ –ø–∞–º—è—Ç—å"""
        print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Ä—Å–∏—é –∏–∑ manifest –µ—Å–ª–∏ –µ—Å—Ç—å
        manifest_path = self.knowledge_dir / 'manifest.json'
        if manifest_path.exists():
            try:
                with open(manifest_path, 'r', encoding='utf-8') as f:
                    manifest = json.load(f)
                    self.kb_version = manifest.get('version', 'unknown')
                    print(f"üì¶ –í–µ—Ä—Å–∏—è –±–∞–∑—ã: {self.kb_version}")
            except:
                self.kb_version = 'unknown'
        
        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name in ['00_index.md', 'build_log.md', 'update_log.md', '_unanswered_log.md']:
                continue
            
            doc = self.extract_metadata_and_content(md_file)
            if doc and 'body' in doc:
                self.documents.append(doc)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n")
    
    def normalize_query(self, query: str) -> List[str]:
        """–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å, –¥–æ–±–∞–≤–∏—Ç—å —Å–∏–Ω–æ–Ω–∏–º—ã"""
        query_lower = query.lower()
        
        # –°–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        synonyms = {
            '–≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–æ': ['citizenship', '–≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–æ', '–ø–∞—Å–ø–æ—Ä—Ç'],
            'citizenship': ['citizenship', '–≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–æ', '–ø–∞—Å–ø–æ—Ä—Ç'],
            '–≤–∏–¥ –Ω–∞ –∂–∏—Ç–µ–ª—å—Å—Ç–≤–æ': ['residence permit', '–≤–∏–¥ –Ω–∞ –∂–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–≤–Ω–∂', 'residence', 'golden visa'],
            'residence': ['residence permit', '–≤–∏–¥ –Ω–∞ –∂–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–≤–Ω–∂', 'residence', 'golden visa'],
            '–≤–Ω–∂': ['residence permit', '–≤–∏–¥ –Ω–∞ –∂–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–≤–Ω–∂', 'residence', 'golden visa'],
            'golden visa': ['golden visa', '–∑–æ–ª–æ—Ç–∞—è –≤–∏–∑–∞', 'residence permit', '–≤–∏–¥ –Ω–∞ –∂–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–≤–Ω–∂'],
            '–∑–æ–ª–æ—Ç–∞—è –≤–∏–∑–∞': ['golden visa', '–∑–æ–ª–æ—Ç–∞—è –≤–∏–∑–∞', 'residence permit', '–≤–∏–¥ –Ω–∞ –∂–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–≤–Ω–∂'],
            '–ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ': ['permanent residence', '–ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ', '–ø–º–∂', 'permanent'],
            'permanent': ['permanent residence', '–ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ', '–ø–º–∂', 'permanent'],
            '–ø–º–∂': ['permanent residence', '–ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ', '–ø–º–∂', 'permanent'],
            '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏': ['investment', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', 'invest'],
            'investment': ['investment', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', 'invest'],
            '–ø–æ–ª—É—á–∏—Ç—å': ['–ø–æ–ª—É—á–∏—Ç—å', 'requirements', 'process', 'procedure', '–∫–∞–∫ –ø–æ–ª—É—á–∏—Ç—å'],
            '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è': ['requirements', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', 'conditions', '—É—Å–ª–æ–≤–∏—è'],
            '—Å—Ç–æ–∏–º–æ—Å—Ç—å': ['cost', 'price', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ü–µ–Ω–∞'],
            'portugal': ['portugal', '–ø–æ—Ä—Ç—É–≥–∞–ª–∏—è'],
            '–ø–æ—Ä—Ç—É–≥–∞–ª–∏—è': ['portugal', '–ø–æ—Ä—Ç—É–≥–∞–ª–∏—è'],
            'malta': ['malta', '–º–∞–ª—å—Ç–∞'],
            '–º–∞–ª—å—Ç–∞': ['malta', '–º–∞–ª—å—Ç–∞'],
            'greece': ['greece', '–≥—Ä–µ—Ü–∏—è'],
            '–≥—Ä–µ—Ü–∏—è': ['greece', '–≥—Ä–µ—Ü–∏—è'],
            'turkey': ['turkey', '—Ç—É—Ä—Ü–∏—è'],
            '—Ç—É—Ä—Ü–∏—è': ['turkey', '—Ç—É—Ä—Ü–∏—è'],
            'caribbean': ['caribbean', '–∫–∞—Ä–∏–±—ã', '–∫–∞—Ä–∏–±—Å–∫–∏–µ –æ—Å—Ç—Ä–æ–≤–∞'],
            '–∫–∞—Ä–∏–±—ã': ['caribbean', '–∫–∞—Ä–∏–±—ã', '–∫–∞—Ä–∏–±—Å–∫–∏–µ –æ—Å—Ç—Ä–æ–≤–∞'],
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = [query_lower]
        
        for key, values in synonyms.items():
            if key in query_lower:
                keywords.extend(values)
        
        return list(set(keywords))
    
    def search_documents(self, query: str, limit: int = 5) -> List[Tuple[Dict, float]]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É (—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)"""
        # –°–æ–∑–¥–∞—ë–º –∫–ª—é—á –∫—ç—à–∞
        cache_key = f"{query.lower().strip()}_{self.kb_version}_{limit}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if cache_key in self.search_cache:
            return self.search_cache[cache_key]
        
        query_lower = query.lower()
        query_words = set(query_lower.split())
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–∞ —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
        query_variants = self.normalize_query(query)
        
        # –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ (—Å —É—á–µ—Ç–æ–º —Ä–∞–∑–Ω—ã—Ö –ø–∞–¥–µ–∂–µ–π)
        countries = {
            'portugal': ['portugal', '–ø–æ—Ä—Ç—É–≥–∞–ª'],  # –ø–æ—Ä—Ç—É–≥–∞–ª–∏—è, –ø–æ—Ä—Ç—É–≥–∞–ª–∏–∏, –ø–æ—Ä—Ç—É–≥–∞–ª–∏—é
            'malta': ['malta', '–º–∞–ª—å—Ç'],  # –º–∞–ª—å—Ç–∞, –º–∞–ª—å—Ç—ã, –º–∞–ª—å—Ç–µ
            'greece': ['greece', '–≥—Ä–µ—Ü–∏'],  # –≥—Ä–µ—Ü–∏—è, –≥—Ä–µ—Ü–∏–∏, –≥—Ä–µ—Ü–∏—é
            'turkey': ['turkey', '—Ç—É—Ä—Ü–∏'],  # —Ç—É—Ä—Ü–∏—è, —Ç—É—Ä—Ü–∏–∏, —Ç—É—Ä—Ü–∏—é
            'grenada': ['grenada', '–≥—Ä–µ–Ω–∞–¥'],  # –≥—Ä–µ–Ω–∞–¥–∞, –≥—Ä–µ–Ω–∞–¥—ã, –≥—Ä–µ–Ω–∞–¥–µ
            'vanuatu': ['vanuatu', '–≤–∞–Ω—É–∞—Ç—É'],  # –Ω–µ —Å–∫–ª–æ–Ω—è–µ—Ç—Å—è
            'antigua': ['antigua', '–∞–Ω—Ç–∏–≥—É–∞'],  # –Ω–µ —Å–∫–ª–æ–Ω—è–µ—Ç—Å—è
            'dominica': ['dominica', '–¥–æ–º–∏–Ω–∏–∫'],  # –¥–æ–º–∏–Ω–∏–∫–∞, –¥–æ–º–∏–Ω–∏–∫–∏
            'st lucia': ['st lucia', '—Å–µ–Ω—Ç-–ª—é—Å–∏', '—Å–µ–Ω—Ç –ª—é—Å–∏'],
            'st kitts': ['st kitts', '—Å–µ–Ω—Ç-–∫–∏—Ç—Å', '—Å–µ–Ω—Ç –∫–∏—Ç—Å'],
            'caribbean': ['caribbean', '–∫–∞—Ä–∏–±'],  # –∫–∞—Ä–∏–±—ã, –∫–∞—Ä–∏–±—Å–∫–∏—Ö
            'cyprus': ['cyprus', '–∫–∏–ø—Ä'],  # –∫–∏–ø—Ä, –∫–∏–ø—Ä–∞
            'spain': ['spain', '–∏—Å–ø–∞–Ω–∏'],  # –∏—Å–ø–∞–Ω–∏—è, –∏—Å–ø–∞–Ω–∏–∏
            'paraguay': ['paraguay', '–ø–∞—Ä–∞–≥–≤–∞'],  # –ø–∞—Ä–∞–≥–≤–∞–π, –ø–∞—Ä–∞–≥–≤–∞—è
            'sao tome': ['sao tome', '—Å–∞–Ω-—Ç–æ–º–µ', '—Å–∞–Ω —Ç–æ–º–µ']
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –≤ –∑–∞–ø—Ä–æ—Å–µ
        country_in_query = None
        for country_key, country_variants in countries.items():
            for variant in country_variants:
                if variant in query_lower:
                    country_in_query = country_key
                    break
            if country_in_query:
                break
        
        results = []
        
        for doc in self.documents:
            body_lower = doc.get('body', '').lower()
            title_lower = doc.get('title', '').lower()
            category_lower = doc.get('category', '').lower()
            subcategory = doc.get('subcategory', '').lower()
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            score = 0
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ –µ—Å—Ç—å —Å—Ç—Ä–∞–Ω–∞ - –¥–∞—ë–º –æ–≥—Ä–æ–º–Ω—ã–π –±—É—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω—ã
            if country_in_query:
                if country_in_query in category_lower or country_in_query in title_lower:
                    score += 500  # –û–≥—Ä–æ–º–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç!
            
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã - –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            for variant in query_variants:
                if variant in body_lower:
                    score += 80
                if variant in title_lower:
                    score += 100
                if variant in category_lower:
                    score += 60
                if variant in subcategory:
                    score += 40
            
            # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞–º
            body_words = set(body_lower.split())
            matching_words = query_words & body_words
            score += len(matching_words) * 15
            
            if score > 0:
                results.append((doc, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x[1], reverse=True)
        final_results = results[:limit]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞)
        if len(self.search_cache) < 100:  # –ú–∞–∫—Å–∏–º—É–º 100 –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            self.search_cache[cache_key] = final_results
        
        return final_results
    
    def extract_relevant_content(self, doc: Dict, query: str, context_size: int = 1000) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        body = doc.get('body', '')
        title = doc.get('title', '')
        summary = doc.get('summary', '')
        
        excerpts = []
        
        # –î–ª—è —Ç–æ–ø–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ—Ä—ë–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç (–∏–ª–∏ –±–æ–ª—å—à—É—é —á–∞—Å—Ç—å)
        # –≠—Ç–æ –¥–∞—Å—Ç LLM –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        
        # –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if title:
            header = f"# {title}\n\n"
            if summary:
                header += f"**–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:** {summary}\n\n"
            excerpts.append(header)
        
        # –ë–µ—Ä—ë–º –≤–µ—Å—å body –¥–æ–∫—É–º–µ–Ω—Ç–∞, —Ä–∞–∑–±–∏–≤–∞—è –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ 2000 —Å–∏–º–≤–æ–ª–æ–≤
        # –≠—Ç–æ –¥–∞—Å—Ç LLM –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if body:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ —Å–ª–∞–π–¥–∞–º
            slides = body.split('--- –°–ª–∞–π–¥')
            
            # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 10 —Å–ª–∞–π–¥–æ–≤ (–æ–±—ã—á–Ω–æ —Ç–∞–º –æ—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
            relevant_slides = []
            for i, slide in enumerate(slides[:15]):  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 15 —Å–ª–∞–π–¥–æ–≤
                if slide.strip():
                    if i > 0:
                        relevant_slides.append('--- –°–ª–∞–π–¥' + slide)
                    else:
                        relevant_slides.append(slide)
            
            if relevant_slides:
                body_excerpt = '\n'.join(relevant_slides)
                # –û–±—Ä–µ–∑–∞–µ–º –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ (–º–∞–∫—Å 6000 —Å–∏–º–≤–æ–ª–æ–≤)
                if len(body_excerpt) > 6000:
                    body_excerpt = body_excerpt[:6000] + "\n\n[...–¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è...]"
                excerpts.append(body_excerpt)
            
            else:
                # Fallback: –ø—Ä–æ—Å—Ç–æ –±–µ—Ä—ë–º –Ω–∞—á–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                excerpts.append(body[:3000])
        
        return excerpts if excerpts else [summary or title or "–ù–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"]
    
    def get_knowledge_base_version(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –≤–µ—Ä—Å–∏—é –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ –∏–Ω–¥–µ–∫—Å–∞"""
        try:
            index_path = self.knowledge_dir / "00_index.md"
            if index_path.exists():
                with open(index_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    match = re.search(r'version:\s*["\']?([^"\'\n]+)["\']?', content)
                    if match:
                        return match.group(1)
            return "unknown"
        except:
            return "unknown"
    
    def extract_keywords(self, text: str, max_keywords: int = 7) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        stop_words = {
            '–≤', '–Ω–∞', '–∏', '–¥–ª—è', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '—Å', '–ø–æ', '–æ', '–∏–∑', '—É',
            'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are'
        }
        
        words = re.findall(r'\b\w+\b', text.lower())
        keywords = [w for w in words if len(w) > 2 and w not in stop_words]
        
        # –ë–µ—Ä—ë–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø–æ –ø–æ—Ä—è–¥–∫—É –ø–æ—è–≤–ª–µ–Ω–∏—è
        seen = set()
        unique_keywords = []
        for kw in keywords:
            if kw not in seen:
                seen.add(kw)
                unique_keywords.append(kw)
        
        return unique_keywords[:max_keywords]
    
    def clean_personal_data(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç –æ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        # –£–¥–∞–ª—è–µ–º email
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text)
        
        # –£–¥–∞–ª—è–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã
        text = re.sub(r'\+?\d[\d\-\(\) ]{7,}\d', '[PHONE]', text)
        
        # –£–¥–∞–ª—è–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ (–∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã)
        # text = re.sub(r'\b[A-Z–ê-–Ø][a-z–∞-—è]+\s+[A-Z–ê-–Ø][a-z–∞-—è]+\b', '[NAME]', text)
        
        return text
    
    def hash_question(self, question: str) -> str:
        """–°–æ–∑–¥–∞—Ç—å —Ö–µ—à –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, –±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        normalized = re.sub(r'[^\w\s]', '', question.lower())
        normalized = ' '.join(normalized.split())  # –£–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        
        hash_obj = hashlib.sha256(normalized.encode('utf-8'))
        return hash_obj.hexdigest()[:8]
    
    def log_unanswered_question(self, question: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–æ—Ç–≤–µ—á–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            current_date = datetime.now().strftime("%Y-%m-%d")
            current_month = datetime.now().strftime("%Y-%m")
            kb_version = self.get_knowledge_base_version()
            question_hash = self.hash_question(question)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self.extract_keywords(question)
            keywords_str = '; '.join(keywords)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É (–ø–µ—Ä–≤—ã–µ 2-3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞)
            topic = ' '.join(keywords[:3]) if keywords else '–û–±—â–∏–π –≤–æ–ø—Ä–æ—Å'
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 —Ä–µ–ø–ª–∏–∫–∏)
            context = ""
            if len(self.conversation_history) >= 2:
                recent = self.conversation_history[-2:]
                context_parts = []
                for q, _ in recent:
                    cleaned = self.clean_personal_data(q)
                    context_parts.append(cleaned[:50])
                context = " ‚Üí ".join(context_parts)
            else:
                context = "–ù–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            if self.unanswered_log_path.exists():
                with open(self.unanswered_log_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # –ò—â–µ–º —ç—Ç–æ—Ç –∂–µ —Ö–µ—à –≤ —Ç–µ–∫—É—â–µ–º –º–µ—Å—è—Ü–µ —Å —Ç–æ–π –∂–µ –≤–µ—Ä—Å–∏–µ–π
                    month_section = re.search(
                        rf'## {re.escape(current_month)}.*?(?=## \d{{4}}-\d{{2}}|$)',
                        content,
                        re.DOTALL
                    )
                    
                    if month_section:
                        month_text = month_section.group(0)
                        if question_hash in month_text and kb_version in month_text:
                            # –î—É–±–ª–∏–∫–∞—Ç –Ω–∞–π–¥–µ–Ω, –Ω–µ –ª–æ–≥–∏—Ä—É–µ–º
                            return
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
            new_entry = f"| {current_date} | {topic} | {question} | [{keywords_str}] | {context} | {kb_version} | {question_hash} |"
            
            # –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
            if self.unanswered_log_path.exists():
                with open(self.unanswered_log_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            else:
                content = "# –í–æ–ø—Ä–æ—Å—ã –±–µ–∑ –æ—Ç–≤–µ—Ç–æ–≤\n\n"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–µ–∫—Ü–∏—è –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –º–µ—Å—è—Ü–∞
            month_header = f"## {current_month}"
            if month_header not in content:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é –º–µ—Å—è—Ü–∞
                table_header = "| –î–∞—Ç–∞ | –¢–µ–º–∞ | –í–æ–ø—Ä–æ—Å | –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ | –ö–æ–Ω—Ç–µ–∫—Å—Ç | –í–µ—Ä—Å–∏—è –±–∞–∑—ã | –•–µ—à –≤–æ–ø—Ä–æ—Å–∞ |\n"
                table_separator = "|------|------|--------|----------------|-----------|--------------|-------------|\n"
                
                new_section = f"\n{month_header}\n\n{table_header}{table_separator}{new_entry}\n"
                content += new_section
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–µ–∫—Ü–∏—é
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Ç–∞–±–ª–∏—Ü—ã —Ç–µ–∫—É—â–µ–≥–æ –º–µ—Å—è—Ü–∞
                pattern = rf'(## {re.escape(current_month)}.*?\|---.*?\|\n)(.*?)(?=\n## |\Z)'
                
                def add_entry(match):
                    header = match.group(1)
                    entries = match.group(2)
                    return header + entries.rstrip() + '\n' + new_entry + '\n'
                
                content = re.sub(pattern, add_entry, content, flags=re.DOTALL)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            line_count = content.count('\n')
            if line_count > 500:
                # –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
                archive_dir = self.knowledge_dir / "_unanswered_archive"
                archive_dir.mkdir(exist_ok=True)
                
                current_year = datetime.now().year
                archive_file = archive_dir / f"{current_year - 1}.md"
                
                # –ü–µ—Ä–µ–Ω–æ—Å–∏–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ (–≤—Å—ë –∫—Ä–æ–º–µ —Ç–µ–∫—É—â–µ–≥–æ –≥–æ–¥–∞)
                current_year_pattern = rf'## {current_year}-\d{{2}}'
                current_year_sections = re.findall(
                    current_year_pattern + r'.*?(?=## \d{4}-\d{2}|$)',
                    content,
                    re.DOTALL
                )
                
                if current_year_sections:
                    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∏–π –≥–æ–¥
                    content = "# –í–æ–ø—Ä–æ—Å—ã –±–µ–∑ –æ—Ç–≤–µ—Ç–æ–≤\n\n" + '\n'.join(current_year_sections)
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ñ–∞–π–ª
            with open(self.unanswered_log_path, 'w', encoding='utf-8') as f:
                f.write(content)
                
        except Exception as e:
            # –ë–µ—Å—à—É–º–Ω–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            pass
    
    def generate_llm_answer(self, query: str, context: str, sources: List[str]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if not self.groq_client:
            return None  # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
        
        try:
            system_prompt = """–¢—ã - –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∞–º –∏–º–º–∏–≥—Ä–∞—Ü–∏–∏ –∏ –ø–æ–ª—É—á–µ–Ω–∏—è –≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–∞.

–í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
1. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ - —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ "–Ω–µ—Ç –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö"
3. –í—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, —Å—É–º–º—ã, —Å—Ä–æ–∫–∏ –∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
4. –§–æ—Ä–º–∞—Ç–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ —Å–ø–∏—Å–∫–∞–º–∏
5. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, —á—Ç–æ –∏ –≤–æ–ø—Ä–æ—Å (—Ä—É—Å—Å–∫–∏–π –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
6. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é - —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –¥–∞—Ç—å –ø–æ–Ω—è—Ç–Ω—ã–π, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""

            user_prompt = f"""–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
{query}

–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:
{context}

–î–∞–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º."""

            response = self.groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",  # –ê–∫—Ç—É–∞–ª—å–Ω–∞—è –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è –º–æ–¥–µ–ª—å Groq
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                max_tokens=1500,
            )
            
            llm_answer = response.choices[0].message.content
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –∫–æ–Ω–µ—Ü
            final_answer = llm_answer + "\n\n---\n\n**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n"
            for source in sources:
                final_answer += f"- {source}\n"
            
            return final_answer
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ LLM: {e}")
            return None  # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
    
    def format_answer(self, query: str, results: List[Tuple[Dict, float]]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not results:
            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–µ–æ—Ç–≤–µ—á–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å
            self.log_unanswered_question(query)
            return "‚ùå –ù–µ –∑–Ω–∞—é ‚Äî –Ω–µ—Ç –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö.\n\n–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
        
        answer_parts = []
        sources = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        seen_excerpts = set()
        
        for doc, score in results[:3]:  # –ë–µ—Ä—ë–º —Ç–æ–ø-3 –¥–æ–∫—É–º–µ–Ω—Ç–∞
            title = doc.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
            category = doc.get('category', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            excerpts = self.extract_relevant_content(doc, query)
            
            for excerpt in excerpts:
                # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                excerpt_normalized = excerpt.lower().strip()
                if excerpt_normalized not in seen_excerpts:
                    seen_excerpts.add(excerpt_normalized)
                    answer_parts.append(excerpt)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
            source_file = doc.get('source_file', 'Unknown')
            slides_start = doc.get('slides_start', '?')
            slides_end = doc.get('slides_end', '?')
            
            source_line = f"raw/{source_file} ‚Üí —Å–ª–∞–π–¥—ã {slides_start}‚Äì{slides_end}"
            if source_line not in sources:
                sources.append(source_line)
        
        # –ü—Ä–æ–±—É–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é LLM
        if self.groq_client and answer_parts:
            context = "\n\n---\n\n".join(answer_parts[:5])  # –¢–æ–ø-5 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            llm_answer = self.generate_llm_answer(query, context, sources)
            if llm_answer:
                return "\n\n## –û—Ç–≤–µ—Ç\n\n" + llm_answer
        
        # Fallback: –æ–±—ã—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –±–µ–∑ LLM
        answer = "\n\n## –û—Ç–≤–µ—Ç\n\n"
        
        if answer_parts:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            for i, part in enumerate(answer_parts[:3], 1):  # –ú–∞–∫—Å–∏–º—É–º 3 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
                if i > 1:
                    answer += "\n\n"
                answer += part
        else:
            answer += "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–∞–π–¥–µ–Ω–∞, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã."
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        answer += "\n\n---\n\n**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n"
        for source in sources:
            answer += f"- {source}\n"
        
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if len(results) > 1:
            answer += "\n*–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.*"
        
        return answer
    
    def ask(self, question: str) -> str:
        """–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∞–≥–µ–Ω—Ç—É"""
        if not question.strip():
            return "‚ùì –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å."
        
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        results = self.search_documents(question)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
        answer = self.format_answer(question, results)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞
        self.conversation_history.append((question, answer))
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self.conversation_history) > 10:
            self.conversation_history = self.conversation_history[-10:]
        
        return answer
    
    def interactive_mode(self):
        """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —á–∞—Ç–∞"""
        print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
        print("‚ïë           –ß–ê–¢-–ê–ì–ï–ù–¢ ‚Äî –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô INTERMARK GLOBAL            ‚ïë")
        print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
        print()
        print("–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –æ–± –∏–º–º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö.")
        print("–ê–≥–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.")
        print()
        print("–ö–æ–º–∞–Ω–¥—ã:")
        print("  /–ø–æ–º–æ—â—å   ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å —Å–ø—Ä–∞–≤–∫—É")
        print("  /—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
        print("  /–≤—ã—Ö–æ–¥    ‚Äî –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞–±–æ—Ç—É")
        print()
        print("‚îÄ" * 64)
        print()
        
        while True:
            try:
                question = input("‚ùì –í–æ–ø—Ä–æ—Å: ").strip()
                
                if not question:
                    continue
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥
                if question.lower() in ['/–≤—ã—Ö–æ–¥', '/exit', '/quit', '–≤—ã—Ö–æ–¥', 'exit']:
                    print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    break
                
                if question.lower() in ['/–ø–æ–º–æ—â—å', '/help', '–ø–æ–º–æ—â—å']:
                    self.show_help()
                    continue
                
                if question.lower() in ['/—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', '/stats', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞']:
                    self.show_stats()
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
                print()
                answer = self.ask(question)
                print(answer)
                print()
                print("‚îÄ" * 64)
                print()
                
            except KeyboardInterrupt:
                print("\n\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            except Exception as e:
                print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞: {e}\n")
    
    def show_help(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø—Ä–∞–≤–∫—É"""
        print()
        print("‚ïê" * 64)
        print("–°–ü–†–ê–í–ö–ê ‚Äî –ß–ê–¢-–ê–ì–ï–ù–¢")
        print("‚ïê" * 64)
        print()
        print("–ê–≥–µ–Ω—Ç –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ:")
        print("  ‚Ä¢ –ü—Ä–æ–≥—Ä–∞–º–º–∞—Ö –≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–∞")
        print("  ‚Ä¢ –í–∏–¥–∞—Ö –Ω–∞ –∂–∏—Ç–µ–ª—å—Å—Ç–≤–æ")
        print("  ‚Ä¢ –ü–æ—Å—Ç–æ—è–Ω–Ω–æ–º –ø—Ä–æ–∂–∏–≤–∞–Ω–∏–∏")
        print("  ‚Ä¢ –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö")
        print("  ‚Ä¢ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –∏ —É—Å–ª–æ–≤–∏—è—Ö –ø—Ä–æ–≥—Ä–∞–º–º")
        print()
        print("–ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:")
        print("  ‚Ä¢ –ö–∞–∫–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–∞ –µ—Å—Ç—å –≤ –ü–æ—Ä—Ç—É–≥–∞–ª–∏–∏?")
        print("  ‚Ä¢ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∏–¥–∞ –Ω–∞ –∂–∏—Ç–µ–ª—å—Å—Ç–≤–æ –≤ –ú–∞–ª—å—Ç–µ?")
        print("  ‚Ä¢ –°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç citizenship by investment –≤ —Å—Ç—Ä–∞–Ω–∞—Ö –ö–∞—Ä–∏–±—Å–∫–æ–≥–æ –±–∞—Å—Å–µ–π–Ω–∞?")
        print("  ‚Ä¢ –ß—Ç–æ —Ç–∞–∫–æ–µ Golden Visa?")
        print()
        print("–ü—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã:")
        print("  ‚úì –¢–æ–ª—å–∫–æ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
        print("  ‚úì –°—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –∫–∞–∂–¥–æ–º –æ—Ç–≤–µ—Ç–µ")
        print("  ‚úì –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî —Å–æ–æ–±—â–∞–µ–º –ø—Ä—è–º–æ")
        print("  ‚úì –ë–µ–∑ —Ñ–∞–Ω—Ç–∞–∑–∏–π –∏ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π")
        print()
        print("‚ïê" * 64)
        print()
    
    def show_stats(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        print()
        print("‚ïê" * 64)
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô")
        print("‚ïê" * 64)
        print()
        print(f"üìö –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}")
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
        categories = set(doc.get('category', 'Unknown') for doc in self.documents)
        print(f"üåç –ö–∞—Ç–µ–≥–æ—Ä–∏–π (—Å—Ç—Ä–∞–Ω): {len(categories)}")
        
        # –¢–∏–ø—ã –ø—Ä–æ–≥—Ä–∞–º–º
        subcategories = defaultdict(int)
        for doc in self.documents:
            subcat = doc.get('subcategory', 'unknown')
            subcategories[subcat] += 1
        
        print()
        print("üìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º:")
        type_names = {
            'citizenship': '–ì—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–æ',
            'residence-permit': '–í–∏–¥ –Ω–∞ –∂–∏—Ç–µ–ª—å—Å—Ç–≤–æ',
            'permanent-residence': '–ü–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ',
            'passport': '–ü–∞—Å–ø–æ—Ä—Ç',
            'golden-visa': '–ó–æ–ª–æ—Ç–∞—è –≤–∏–∑–∞',
            'general': '–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'
        }
        
        for subcat, count in sorted(subcategories.items(), key=lambda x: x[1], reverse=True):
            name = type_names.get(subcat, subcat)
            print(f"  ‚Ä¢ {name}: {count}")
        
        print()
        print("‚ïê" * 64)
        print()


def main():
    base_dir = Path(__file__).parent
    knowledge_dir = base_dir / "knowledge"
    
    if not knowledge_dir.exists():
        print("‚ùå –ü–∞–ø–∫–∞ knowledge/ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞: python3 build_knowledge.py")
        sys.exit(1)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    md_files = list(knowledge_dir.rglob("*.md"))
    doc_files = [f for f in md_files if f.name not in ['00_index.md', 'build_log.md', 'update_log.md']]
    
    if not doc_files:
        print("‚ùå –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞.")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python3 build_knowledge.py")
        sys.exit(1)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–≥–µ–Ω—Ç–∞
    agent = KnowledgeAgent(knowledge_dir)
    
    # –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã
    if len(sys.argv) > 1:
        # –ö–æ–º–∞–Ω–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ - –æ–¥–∏–Ω –≤–æ–ø—Ä–æ—Å
        question = ' '.join(sys.argv[1:])
        print(f"‚ùì –í–æ–ø—Ä–æ—Å: {question}\n")
        answer = agent.ask(question)
        print(answer)
    else:
        # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
        agent.interactive_mode()


if __name__ == "__main__":
    main()


#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ PDF –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–π
–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ—Ç –ø–æ —Ç–µ–º–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
–í–µ—Ä—Å–∏—è 2.0 - —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–≤—É—è–∑—ã—á–Ω–æ—Å—Ç–∏ (rus/eng)
"""

import os
import sys
import hashlib
import re
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Literal
from concurrent.futures import ThreadPoolExecutor, as_completed
import shutil

try:
    import PyPDF2
except ImportError:
    print("–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫...")
    os.system(f"{sys.executable} -m pip install PyPDF2 --quiet")
    import PyPDF2

# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–∞–º–∏
try:
    from language_utils import LanguageDetector, LanguageRouter, Language
    LANGUAGE_UTILS_AVAILABLE = True
except ImportError:
    LANGUAGE_UTILS_AVAILABLE = False
    Language = Literal["rus", "eng"]
    print("‚ö†Ô∏è  language_utils –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–≤—É—è–∑—ã—á–Ω–æ—Å—Ç—å –æ—Ç–∫–ª—é—á–µ–Ω–∞.")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
MAX_WORKERS = min(os.cpu_count() - 1 if os.cpu_count() else 1, 6)
BATCH_SIZE = 4


class KnowledgeBaseBuilder:
    def __init__(self, raw_dir: str, knowledge_dir: str, bilingual: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è builder
        
        Args:
            raw_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ PDF —Ñ–∞–π–ª–∞–º–∏
            knowledge_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
            bilingual: –í–∫–ª—é—á–∏—Ç—å –¥–≤—É—è–∑—ã—á–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É (rus/eng –æ—Ç–¥–µ–ª—å–Ω–æ)
        """
        self.raw_dir = Path(raw_dir)
        self.knowledge_dir = Path(knowledge_dir)
        self.build_date = datetime.now().strftime("%Y-%m-%d")
        self.bilingual = bilingual and LANGUAGE_UTILS_AVAILABLE
        self.documents = []
        self.build_log = []
        
        # –î–≤—É—è–∑—ã—á–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
        self.documents_by_lang = {'rus': [], 'eng': []} if self.bilingual else None
        
        # –£—Ç–∏–ª–∏—Ç—ã —è–∑—ã–∫–∞
        if self.bilingual:
            self.language_detector = LanguageDetector()
            self.language_router = LanguageRouter(self.knowledge_dir)
            self.log("üåç –î–≤—É—è–∑—ã—á–Ω—ã–π —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω (RUS/ENG)")
        else:
            self.language_detector = None
            self.language_router = None
            self.log("üìö –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º (legacy)")
        
    def log(self, message: str):
        """–î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø–∏—Å—å –≤ –ª–æ–≥"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.build_log.append(log_entry)
        print(log_entry)
    
    def calculate_checksum(self, file_path: Path) -> Tuple[str, int, float]:
        """–í—ã—á–∏—Å–ª–∏—Ç—å SHA256 —Ö–µ—à —Ñ–∞–π–ª–∞ + —Ä–∞–∑–º–µ—Ä + mtime"""
        sha256 = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b""):
                sha256.update(chunk)
        stat = file_path.stat()
        return sha256.hexdigest()[:16], stat.st_size, stat.st_mtime
    
    def extract_text_from_pdf(self, pdf_path: Path) -> Tuple[str, int]:
        """–ò–∑–≤–ª–µ—á—å –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞"""
        text_content = []
        page_count = 0
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                page_count = len(pdf_reader.pages)
                
                for page_num, page in enumerate(pdf_reader.pages, 1):
                    page_text = page.extract_text()
                    if page_text.strip():
                        # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                        text_content.append(f"--- –°–ª–∞–π–¥ {page_num} ---")
                        text_content.append(page_text.strip())
                        text_content.append("")
                        
        except Exception as e:
            self.log(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {pdf_path.name}: {e}")
            return "", 0
        
        return "\n".join(text_content), page_count
    
    def detect_document_language(self, pdf_path: Path, content: str) -> Language:
        """
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        –ü—Ä–∞–≤–∏–ª–∞:
        1. –ï—Å–ª–∏ —Ñ–∞–π–ª –≤ raw/rus/ ‚Üí rus
        2. –ï—Å–ª–∏ —Ñ–∞–π–ª –≤ raw/eng/ ‚Üí eng
        3. –ï—Å–ª–∏ legacy (raw/*.pdf) ‚Üí –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        
        Args:
            pdf_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
            content: –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        
        Returns:
            "rus" –∏–ª–∏ "eng"
        """
        if not self.bilingual or not self.language_detector:
            return "rus"  # Default –¥–ª—è legacy —Ä–µ–∂–∏–º–∞
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        source_lang = self.language_detector.get_source_language(pdf_path)
        if source_lang:
            return source_lang
        
        # Legacy —Ñ–∞–π–ª - –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        detected = self.language_detector.detect_from_text(content, threshold=0.30)
        self.log(f"  üìù Legacy —Ñ–∞–π–ª {pdf_path.name} ‚Üí –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –∫–∞–∫ {detected.upper()}")
        return detected
    
    def categorize_file(self, filename: str, content: str, lang: Optional[Language] = None) -> Dict[str, any]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        filename_lower = filename.lower()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä–∞–Ω—É/—Ä–µ–≥–∏–æ–Ω –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        # –§–æ—Ä–º–∞—Ç: "Intermark. <Country/Region> <Program Type> ENG.pdf"
        match = re.match(r'Intermark\.\s+(.+?)\s+(Residence|Citizenship|Passport|visa|RP|PR|Permit)', 
                        filename, re.IGNORECASE)
        
        if match:
            country = match.group(1).strip()
        else:
            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å —Å—Ç—Ä–∞–Ω—É –∏–∑ –Ω–∞—á–∞–ª–∞ –∏–º–µ–Ω–∏
            parts = filename.replace("Intermark.", "").replace("ENG.pdf", "").strip().split(".")
            country = parts[0].strip() if parts else "–û–±—â–µ–µ"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø—Ä–æ–≥—Ä–∞–º–º—ã
        if any(word in filename_lower for word in ['citizenship', '–≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–æ']):
            program_type = '–ì—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–æ'
            subcategory = 'citizenship'
        elif any(word in filename_lower for word in ['permanent residence', 'pr']):
            program_type = '–ü–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ'
            subcategory = 'permanent-residence'
        elif any(word in filename_lower for word in ['residence permit', 'rp', 'visa']):
            program_type = '–í–∏–¥ –Ω–∞ –∂–∏—Ç–µ–ª—å—Å—Ç–≤–æ'
            subcategory = 'residence-permit'
        elif 'passport' in filename_lower:
            program_type = '–ü–∞—Å–ø–æ—Ä—Ç'
            subcategory = 'passport'
        elif 'golden visa' in filename_lower:
            program_type = '–ó–æ–ª–æ—Ç–∞—è –≤–∏–∑–∞'
            subcategory = 'golden-visa'
        else:
            program_type = '–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'
            subcategory = 'general'
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        lines = [l.strip() for l in content.split('\n') if l.strip() and not l.startswith('---')]
        summary = ' '.join(lines[:3])[:200] if lines else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–º–º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º–µ"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–≥–∏
        tags = [country, program_type]
        if 'investment' in filename_lower or 'invest' in content.lower()[:500]:
            tags.append('–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏')
        if 'startup' in filename_lower:
            tags.append('—Å—Ç–∞—Ä—Ç–∞–ø')
        if 'digital nomad' in filename_lower:
            tags.append('digital-nomad')
        if 'financial independence' in filename_lower:
            tags.append('—Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è-–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å')
        
        result = {
            'country': country,
            'program_type': program_type,
            'subcategory': subcategory,
            'summary': summary,
            'tags': tags
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º —è–∑—ã–∫ –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω
        if lang:
            result['lang'] = lang
        
        return result
    
    def _get_source_path(self, pdf_file: Path, lang: Optional[Language] = None) -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø—É—Ç—å –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É —Å —É—á–µ—Ç–æ–º —è–∑—ã–∫–∞
        
        Args:
            pdf_file: Path –æ–±—ä–µ–∫—Ç –∫ PDF —Ñ–∞–π–ª—É
            lang: –Ø–∑—ã–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Returns:
            –ü—É—Ç—å –≤–∏–¥–∞ "raw/{lang}/file.pdf" –∏–ª–∏ "raw/file.pdf"
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Ñ–∞–π–ª —É–∂–µ –≤ —è–∑—ã–∫–æ–≤–æ–π –ø–∞–ø–∫–µ
        parts = pdf_file.parts
        if 'rus' in parts or 'eng' in parts:
            # –§–∞–π–ª —É–∂–µ –≤ —è–∑—ã–∫–æ–≤–æ–π –ø–∞–ø–∫–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
            raw_index = parts.index('raw')
            return '/'.join(parts[raw_index:])
        
        # Legacy —Ñ–∞–π–ª - —Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å —Å —É—á–µ—Ç–æ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞
        if lang and self.bilingual:
            return f"raw/{lang}/{pdf_file.name}"
        else:
            return f"raw/{pdf_file.name}"
    
    def create_markdown_document(self, pdf_file: Path, text: str, page_count: int, 
                                 metadata: Dict) -> str:
        """–°–æ–∑–¥–∞—Ç—å Markdown –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ —à–∞–±–ª–æ–Ω—É"""
        checksum = self.calculate_checksum(pdf_file)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è markdown
        safe_name = re.sub(r'[^\w\s-]', '', metadata['country'])
        safe_name = re.sub(r'[-\s]+', '-', safe_name).strip('-').lower()
        md_filename = f"{safe_name}-{metadata['subcategory']}.md"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É —Å —É—á–µ—Ç–æ–º —è–∑—ã–∫–∞
        source_path = self._get_source_path(pdf_file, metadata.get('lang'))
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–µ lang –¥–ª—è YAML (–µ—Å–ª–∏ –µ—Å—Ç—å)
        lang_field = f'lang: "{metadata["lang"]}"\n' if 'lang' in metadata else ''
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        doc = f"""---
title: "{metadata['country']}: {metadata['program_type']}"
summary: "{metadata['summary']}"
category: "{metadata['country']}"
subcategory: "{metadata['subcategory']}"
{lang_field}tags: {metadata['tags']}
source_files:
  - path: "{source_path}"
    slides: [1-{page_count}]
extraction_date: "{self.build_date}"
version: "{checksum}"
checksum_sources: "{checksum}"
doc_type: "knowledge"
related: []
---

# {metadata['country']}: {metadata['program_type']}

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏

{text}

---

### –ò—Å—Ç–æ—á–Ω–∏–∫–∏
[^src1]: raw/{pdf_file.name} ‚Üí —Å–ª–∞–π–¥—ã 1‚Äì{page_count}
"""
        
        return md_filename, doc
    
    def clean_knowledge_dir(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –ø–∞–ø–∫—É knowledge –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
        if self.knowledge_dir.exists():
            self.log(f"üóëÔ∏è  –û—á–∏—Å—Ç–∫–∞ –ø–∞–ø–∫–∏ {self.knowledge_dir}")
            shutil.rmtree(self.knowledge_dir)
        
        self.knowledge_dir.mkdir(parents=True, exist_ok=True)
        self.log(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ {self.knowledge_dir}")
    
    def process_single_file(self, pdf_file: Path, file_index: int, total_files: int) -> Tuple[Dict, str]:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω PDF —Ñ–∞–π–ª (–¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è)"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text, page_count = self.extract_text_from_pdf(pdf_file)
            
            if not text:
                return None, f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—Å–∫ {pdf_file.name}: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç"
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_lang = self.detect_document_language(pdf_file, text)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é (—Å —è–∑—ã–∫–æ–º)
            metadata = self.categorize_file(pdf_file.name, text, lang=doc_lang)
            
            # –°–æ–∑–¥–∞–µ–º markdown –¥–æ–∫—É–º–µ–Ω—Ç
            md_filename, md_content = self.create_markdown_document(
                pdf_file, text, page_count, metadata
            )
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é (—Å —É—á–µ—Ç–æ–º —è–∑—ã–∫–∞)
            if self.bilingual and self.language_router:
                base_dir = self.language_router.get_docs_dir(doc_lang)
            else:
                base_dir = self.knowledge_dir
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å—Ç—Ä–∞–Ω—ã
            country_dir = base_dir / metadata['country']
            country_dir.mkdir(exist_ok=True, parents=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
            md_path = country_dir / md_filename
            version = 2
            while md_path.exists():
                base_name = md_filename.replace('.md', '')
                md_filename = f"{base_name}_v{version}.md"
                md_path = country_dir / md_filename
                version += 1
            
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(md_content)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ knowledge/)
            if self.bilingual and self.language_router:
                # –ü—É—Ç—å –≤–∫–ª—é—á–∞–µ—Ç —è–∑—ã–∫–æ–≤—É—é –ø–∞–ø–∫—É: rus/Country/file.md
                doc_path = f"{doc_lang}/{metadata['country']}/{md_filename}"
            else:
                # Legacy –ø—É—Ç—å: Country/file.md
                doc_path = f"{metadata['country']}/{md_filename}"
            
            # –ì–æ—Ç–æ–≤–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            doc_info = {
                'country': metadata['country'],
                'title': f"{metadata['country']}: {metadata['program_type']}",
                'path': doc_path,
                'summary': metadata['summary'],
                'subcategory': metadata['subcategory'],
                'tags': metadata['tags'],
                'source_file': str(pdf_file.name),
                'checksum': self.calculate_checksum(pdf_file)[0],
                'lang': doc_lang if self.bilingual else None
            }
            
            return doc_info, f"‚úÖ ({doc_lang.upper()}) –°–æ–∑–¥–∞–Ω: {doc_path}"
            
        except Exception as e:
            return None, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {pdf_file.name}: {e}"
    
    def process_all_files(self):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ PDF —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ raw (—Å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–æ–º)"""
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª—ã –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        pdf_files = []
        
        # Legacy —Ñ–∞–π–ª—ã –≤ –∫–æ—Ä–Ω–µ raw/
        pdf_files.extend(list(self.raw_dir.glob("*.pdf")))
        
        # –§–∞–π–ª—ã –≤ —è–∑—ã–∫–æ–≤—ã—Ö –ø–∞–ø–∫–∞—Ö (–µ—Å–ª–∏ –¥–≤—É—è–∑—ã—á–Ω—ã–π —Ä–µ–∂–∏–º)
        if self.bilingual:
            rus_dir = self.raw_dir / 'rus'
            eng_dir = self.raw_dir / 'eng'
            
            if rus_dir.exists():
                pdf_files.extend(list(rus_dir.glob("*.pdf")))
            
            if eng_dir.exists():
                pdf_files.extend(list(eng_dir.glob("*.pdf")))
        
        pdf_files = sorted(pdf_files)
        
        if not pdf_files:
            self.log("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ PDF —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ raw/")
            return
        
        self.log(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(pdf_files)} PDF —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        self.log(f"‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {min(MAX_WORKERS, len(pdf_files))} –ø–æ—Ç–æ–∫–æ–≤")
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º
        countries = {}
        manifest_sources = []
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
        with ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(pdf_files))) as executor:
            futures = {}
            for i, pdf_file in enumerate(pdf_files):
                future = executor.submit(self.process_single_file, pdf_file, i + 1, len(pdf_files))
                futures[future] = pdf_file
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
            completed = 0
            for future in as_completed(futures):
                pdf_file = futures[future]
                completed += 1
                
                try:
                    doc_info, log_message = future.result()
                    
                    if doc_info:
                        self.documents.append(doc_info)
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —è–∑—ã–∫–æ–≤—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é (–µ—Å–ª–∏ –¥–≤—É—è–∑—ã—á–Ω—ã–π —Ä–µ–∂–∏–º)
                        if self.bilingual and doc_info.get('lang'):
                            doc_lang = doc_info['lang']
                            self.documents_by_lang[doc_lang].append(doc_info)
                        
                        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å—Ç—Ä–∞–Ω–∞–º
                        country = doc_info['country']
                        if country not in countries:
                            countries[country] = []
                        countries[country].append(doc_info)
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ manifest
                        checksum, size, mtime = self.calculate_checksum(pdf_file)
                        manifest_sources.append({
                            'path': f"raw/{pdf_file.name}",
                            'sha256': checksum,
                            'size': size,
                            'mtime': mtime
                        })
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 10 —Ñ–∞–π–ª–æ–≤ –∏–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π
                    if completed % 10 == 0 or completed == len(pdf_files):
                        self.log(f"üìÑ [{completed}/{len(pdf_files)}] {log_message}")
                    
                except Exception as e:
                    self.log(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {pdf_file.name}: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º manifest
        self.save_manifest(manifest_sources)
        
        return countries
    
    def create_index(self, countries: Dict):
        """–°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å–Ω—ã–π —Ñ–∞–π–ª 00_index.md"""
        self.log("üìë –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞...")
        
        index_content = f"""---
title: "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π ‚Äî –ò–º–º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã"
type: "index"
created: "{self.build_date}"
total_documents: {len(self.documents)}
total_categories: {len(countries)}
version: "{hashlib.md5(str(self.documents).encode()).hexdigest()[:8]}"
---

# –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π ‚Äî –ò–º–º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã

–ü–æ–ª–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–º–º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º Intermark Global.

**–û–±–Ω–æ–≤–ª–µ–Ω–æ:** {self.build_date}  
**–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:** {len(self.documents)}  
**–ö–∞—Ç–µ–≥–æ—Ä–∏–π (—Å—Ç—Ä–∞–Ω/—Ä–µ–≥–∏–æ–Ω–æ–≤):** {len(countries)}

---

## –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º

"""
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω—ã –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
        for country in sorted(countries.keys()):
            docs = countries[country]
            index_content += f"\n### {country}\n\n"
            
            for doc in sorted(docs, key=lambda x: x['subcategory']):
                index_content += f"- [{doc['title']}]({doc['path']})\n"
                index_content += f"  - *{doc['summary'][:100]}...*\n"
        
        index_content += f"""

---

## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

- –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(self.documents)}
- –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {len(countries)}
- –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {self.build_date}

---

*–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–π Intermark Global*
"""
        
        index_path = self.knowledge_dir / "00_index.md"
        with open(index_path, 'w', encoding='utf-8') as f:
            f.write(index_content)
        
        self.log(f"‚úÖ –°–æ–∑–¥–∞–Ω –∏–Ω–¥–µ–∫—Å–Ω—ã–π —Ñ–∞–π–ª: 00_index.md")
    
    def save_manifest(self, sources: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å manifest.json (–∏ —è–∑—ã–∫–æ–≤—ã–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã –µ—Å–ª–∏ –¥–≤—É—è–∑—ã—á–Ω—ã–π —Ä–µ–∂–∏–º)"""
        try:
            version = f"build_{self.build_date}_{datetime.now().strftime('%H-%M')}"
            
            # –û–±—â–∏–π manifest (legacy —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
            manifest = {
                'version': version,
                'created': datetime.now().isoformat(),
                'sources': sources,
                'total_documents': len(self.documents),
                'total_sources': len(sources)
            }
            
            manifest_path = self.knowledge_dir / 'manifest.json'
            with open(manifest_path, 'w', encoding='utf-8') as f:
                json.dump(manifest, f, indent=2, ensure_ascii=False)
            
            self.log(f"‚úÖ –°–æ–∑–¥–∞–Ω manifest: {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            
            # –Ø–∑—ã–∫–æ–≤—ã–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç—ã (–µ—Å–ª–∏ –¥–≤—É—è–∑—ã—á–Ω—ã–π —Ä–µ–∂–∏–º)
            if self.bilingual and self.documents_by_lang and self.language_router:
                for lang in ['rus', 'eng']:
                    lang_docs = self.documents_by_lang[lang]
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ —è–∑—ã–∫—É
                    lang_sources = [
                        s for s in sources 
                        if any(doc['source_file'] == Path(s['path']).name and doc.get('lang') == lang 
                               for doc in lang_docs)
                    ]
                    
                    lang_manifest = {
                        'version': version,
                        'created': datetime.now().isoformat(),
                        'language': lang,
                        'sources': lang_sources,
                        'total_documents': len(lang_docs),
                        'total_sources': len(lang_sources)
                    }
                    
                    lang_manifest_path = self.language_router.get_manifest_path(lang)
                    with open(lang_manifest_path, 'w', encoding='utf-8') as f:
                        json.dump(lang_manifest, f, indent=2, ensure_ascii=False)
                    
                    self.log(f"‚úÖ –°–æ–∑–¥–∞–Ω manifest.{lang}.json: {len(lang_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
        except Exception as e:
            self.log(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è manifest: {e}")
    
    def create_build_log(self):
        """–°–æ–∑–¥–∞—Ç—å –ª–æ–≥ —Å–±–æ—Ä–∫–∏"""
        self.log("üìã –°–æ–∑–¥–∞–Ω–∏–µ –ª–æ–≥–∞ —Å–±–æ—Ä–∫–∏...")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —è–∑—ã–∫–∞–º
        lang_stats = ""
        if self.bilingual and self.documents_by_lang:
            lang_stats = "\n### –Ø–∑—ã–∫–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n"
            for lang in ['rus', 'eng']:
                count = len(self.documents_by_lang[lang])
                lang_stats += f"- **{lang.upper()}:** {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n"
        
        log_content = f"""# –õ–æ–≥ —Å–±–æ—Ä–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

**–î–∞—Ç–∞:** {self.build_date}  
**–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞:** {self.build_log[0].split(']')[0].strip('[')}  
**–í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è:** {self.build_log[-1].split(']')[0].strip('[')}

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

- **–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤:** {len(self.documents)}
- **–°–æ–∑–¥–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:** {len(self.documents)}
- **–ö–∞—Ç–µ–≥–æ—Ä–∏–π:** {len(set(doc['country'] for doc in self.documents))}
{lang_stats}
## –ü–æ–¥—Ä–æ–±–Ω—ã–π –ª–æ–≥

"""
        
        for entry in self.build_log:
            log_content += f"{entry}\n"
        
        log_path = self.knowledge_dir / "build_log.md"
        with open(log_path, 'w', encoding='utf-8') as f:
            f.write(log_content)
        
        self.log(f"‚úÖ –°–æ–∑–¥–∞–Ω –ª–æ–≥ —Å–±–æ—Ä–∫–∏: build_log.md")
    
    def build(self):
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Å–±–æ—Ä–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        self.log("üöÄ –ù–∞—á–∞–ª–æ —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
        self.log(f"üìÇ –ò—Å—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {self.raw_dir}")
        self.log(f"üìÇ –¶–µ–ª–µ–≤–∞—è –ø–∞–ø–∫–∞: {self.knowledge_dir}")
        
        # –®–∞–≥ 0: –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π (–µ—Å–ª–∏ –¥–≤—É—è–∑—ã—á–Ω—ã–π —Ä–µ–∂–∏–º)
        if self.bilingual and self.language_router:
            self.language_router.ensure_structure()
            self.log("‚úÖ –î–≤—É—è–∑—ã—á–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –≥–æ—Ç–æ–≤–∞")
        
        # –®–∞–≥ 1: –û—á–∏—Å—Ç–∫–∞ (–ù–ï –æ—á–∏—â–∞–µ–º —è–∑—ã–∫–æ–≤—ã–µ –ø–∞–ø–∫–∏, —Ç–æ–ª—å–∫–æ legacy)
        # self.clean_knowledge_dir() - –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        
        # –®–∞–≥ 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
        countries = self.process_all_files()
        
        if not countries:
            self.log("‚ùå –ù–µ —Å–æ–∑–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            return
        
        # –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        self.create_index(countries)
        
        # –®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ –ª–æ–≥–∞
        self.create_build_log()
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.log("")
        self.log("=" * 60)
        self.log(f"‚úÖ –ó–ê–í–ï–†–®–ï–ù–û: –°–æ–∑–¥–∞–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        self.log(f"üìä –ö–∞—Ç–µ–≥–æ—Ä–∏–π (—Å—Ç—Ä–∞–Ω): {len(countries)}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —è–∑—ã–∫–∞–º
        if self.bilingual and self.documents_by_lang:
            self.log(f"üåç –Ø–∑—ã–∫–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            for lang in ['rus', 'eng']:
                count = len(self.documents_by_lang[lang])
                if count > 0:
                    self.log(f"   ‚Ä¢ {lang.upper()}: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        self.log(f"üìÖ –î–∞—Ç–∞ —Å–±–æ—Ä–∫–∏: {self.build_date}")
        self.log("=" * 60)


def main():
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏
    base_dir = Path(__file__).parent
    raw_dir = base_dir / "raw"
    knowledge_dir = base_dir / "knowledge"
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Å–±–æ—Ä—â–∏–∫
    builder = KnowledgeBaseBuilder(raw_dir, knowledge_dir)
    builder.build()


if __name__ == "__main__":
    main()

